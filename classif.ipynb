{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phishing Email Generation and Classification\n",
    "\n",
    "Experimental notebook for classification and generation of phishing emails using several techniques.\n",
    "\n",
    "Roadmap:\n",
    "- Preprocessing\n",
    "    - Sentiment Analysis\n",
    "- Classification\n",
    "    - Logistic Regression on emails\n",
    "    - word2vec model\n",
    "    - Transformer-based model\n",
    "- Generation\n",
    "    - With GANs\n",
    "    - With T5\n",
    "    - With transformers\n",
    "- Adversarial Learning\n",
    "    - EA-based method of classifying\n",
    "    - Cyclical GANs\n",
    "    - Use EA to seed an NN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements - uncomment this line the first time you run this notebook\n",
    "#!pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "myseed = 10897"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email-Spam-Dataset\n",
    "\n",
    "Use this dataset as preliminary exploration of classification techniques. Classifier can then be applied to Enron dataset if it works\n",
    "\n",
    "0: not spam\n",
    "1: spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_col_names = ['id', 'Body', 'Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSA = pd.read_csv('./kaggle-datasets/Email-Spam-Dataset/completeSpamAssassin.csv', names=spam_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEnron = pd.read_csv('./kaggle-datasets/Email-Spam-Dataset/enronSpamSubset.csv', names=spam_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLing = pd.read_csv('./kaggle-datasets/Email-Spam-Dataset/lingSpam.csv', names=spam_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge datasets\n",
    "dfs = [dfSA, dfEnron, dfLing]\n",
    "dfSpam = pd.concat(dfs).dropna(axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a peek at the body of the emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>\\r\\nSave up to 70% on Life Insurance.\\r\\nWhy S...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\r\\nhttp://www.adc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\r\\nhttp://www.adc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>##############################################...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I thought you might like these:\\r\\n1) Slim Dow...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               Body Label\n",
       "1.0  0.0  \\r\\nSave up to 70% on Life Insurance.\\r\\nWhy S...     1\n",
       "2.0  1.0  1) Fight The Risk of Cancer!\\r\\nhttp://www.adc...     1\n",
       "3.0  2.0  1) Fight The Risk of Cancer!\\r\\nhttp://www.adc...     1\n",
       "4.0  3.0  ##############################################...     1\n",
       "5.0  4.0  I thought you might like these:\\r\\n1) Slim Dow...     1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSpam.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In avoidance of 'self-plagiarisation', much of the code for the `Dataset` class is adapted from my Text Processing Sentiment Analysis assignment. Code can be provided to markers on request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords as NLTK_STOP\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download wordnet for lemmatization\n",
    "#uncomment appropriate line if you get error: \"Resource wordnet not found.\", \"Resource punkt not found.\", etc...\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy imports\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOP\n",
    "\n",
    "#download spacy dataset\n",
    "#uncomment the line below if you get error \"Can't find model 'en_core_web_sm'\"\n",
    "# ! python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, df, preprocessing=[\"lower\"], feature_selection=[\"alltokens\"], retList=False):\n",
    "        \n",
    "        #read preprocessing and feature_selection configuration\n",
    "        self.preprocessing = preprocessing\n",
    "        self.feature_selection = feature_selection\n",
    "        self.data = df\n",
    "        self.retList = retList\n",
    "\n",
    "        # === define various processors and regexes for various preprocessing/feature selection methods ===\n",
    "\n",
    "        # NLTK Stemming Engine\n",
    "        self.porter = PorterStemmer()\n",
    "\n",
    "        #NLTK Lemmatizing Engine\n",
    "        self.wn_lt = WordNetLemmatizer()\n",
    "\n",
    "        #tokenizers\n",
    "\n",
    "        #words regex - splits on word boundaries, doesn't include punctuation etc\n",
    "        self.word_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "        #create tokenizer based on NLTK-provided regex from Labs\n",
    "        nltk_pat = r'''(?x) # set flag to allow verbose regexps\n",
    "            (?:[A-Z]\\.)+ # abbreviations, e.g. U.S.A.\n",
    "            | \\w+(?:-\\w+)* # words with optional internal hyphens\n",
    "            | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "            | \\.\\.\\. # ellipsis\n",
    "            | [][.,;\"'?():_`-]\n",
    "            | [>]?[:;][\\']?[\\(\\)\\[\\]]+ # these are separate tokens; includes ], [\n",
    "            '''\n",
    "        self.nltk_tokenizer = RegexpTokenizer(nltk_pat)\n",
    "\n",
    "        #create tokenizer based on custom regex based on above, with less features\n",
    "        custom_pat = r'''(?x)\n",
    "                \\w+(?:-\\w+)*\n",
    "                |\\$?\\d+(?:\\.\\d+)?%?\n",
    "                |\\.\\.\\.'''\n",
    "        self.custom_tokenizer = RegexpTokenizer(custom_pat)\n",
    "\n",
    "        # stop words - SPACY_STOP defined above in imports\n",
    "\n",
    "        self.NLTK_ENGLISH_STOP = set(NLTK_STOP.words('english'))\n",
    "\n",
    "        # === apply preprocessing and feature selection ===\n",
    "\n",
    "        self.process_phrases()\n",
    "    \n",
    "    def preprocess_phrase(self, phrase):\n",
    "        \"\"\"define preprocessing function for phrases\n",
    "        can call any number of these options - however, some may not combine well\"\"\"\n",
    "\n",
    "        if self.preprocessing == []:\n",
    "            #no preprocessing\n",
    "            return phrase\n",
    "        if \"lower\" in self.preprocessing:\n",
    "            #lowercase\n",
    "            phrase = phrase.lower()\n",
    "        if \"newlines\" in self.preprocessing:\n",
    "            phrase = \" \".join(filter(None, phrase.split(\"\\n\")))\n",
    "        if \"punc\" in self.preprocessing:\n",
    "            #remove punctuation\n",
    "            phrase = phrase.translate(str.maketrans('','',string.punctuation))\n",
    "        if \"stemming\" in self.preprocessing:\n",
    "            #use NLTK stemming\n",
    "            phrase = self.porter.stem(phrase)\n",
    "        if \"nltk_lemmatize\" in self.preprocessing:\n",
    "            #use NLTK's lemmatization method\n",
    "            new_phrase = \"\"\n",
    "            words = nltk.word_tokenize(phrase)\n",
    "            for word in words:\n",
    "                new_phrase += self.wn_lt.lemmatize(word)\n",
    "            phrase = new_phrase\n",
    "        if \"spacy_lemmatize\" in self.preprocessing:\n",
    "            #use spacy's lemmatization method\n",
    "            nlp_phrase = nlp(phrase)\n",
    "            new_phrase = \"\"\n",
    "            for token in nlp_phrase:\n",
    "                new_phrase += (token.lemma_ + \" \")\n",
    "            phrase = new_phrase\n",
    "\n",
    "        return phrase\n",
    "\n",
    "    def extract_features_from_phrase(self, phrase):\n",
    "        \"\"\"define feature extraction function for phrases\n",
    "        extracts all words from all phrases as features for document set\n",
    "        for each method, if one has already been applied the phrase must be treated as a list\"\"\"\n",
    "\n",
    "        #list of negation words from \n",
    "        negation_words = [\"neither\", \"never\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"nowhere\"]\n",
    "\n",
    "        intensifer_words = [\"absolutely\", \"completely\", \"extremely\", \"highly\", \"rather\", \"really\", \"so\", \"too\", \"totally\", \"utterly\", \"very\"]\n",
    "\n",
    "        #use if len(self.feature_selection) > 1 to check if any preprocessing already occurred, as input will be in a list\n",
    "\n",
    "        if self.feature_selection == []:\n",
    "            #we have to do some feature selection, absolute minimum is alltokens\n",
    "            return phrase.split(\" \")\n",
    "        if \"alltokens\" in self.feature_selection:\n",
    "            #split based on whitespace\n",
    "            phrase = phrase.split(\" \")\n",
    "        if \"nltk_tokenize\" in self.feature_selection:\n",
    "            #tokenize using NLTK tokenizer, with words regex\n",
    "            if isinstance(phrase, list):\n",
    "                phrase =  list(itertools.chain.from_iterable([self.word_tokenizer.tokenize(partial_phrase) for partial_phrase in phrase]))\n",
    "            else:\n",
    "                phrase = self.word_tokenizer.tokenize(phrase)\n",
    "                # print(phrase)\n",
    "        if \"nltk_tokenize_2\" in self.feature_selection:\n",
    "            #tokenize using NLTK's regex\n",
    "            if isinstance(phrase, list):\n",
    "                phrase =  list(itertools.chain.from_iterable([self.nltk_tokenizer.tokenize(partial_phrase) for partial_phrase in phrase]))\n",
    "            else:\n",
    "                phrase = self.nltk_tokenizer.tokenize(phrase)\n",
    "        if \"custom_tokenize\" in self.feature_selection:\n",
    "            #tokenize with custom regex\n",
    "            if isinstance(phrase, list):\n",
    "                phrase = list(itertools.chain.from_iterable([self.custom_tokenizer.tokenize(partial_phrase) for partial_phrase in phrase]))\n",
    "            else:\n",
    "                phrase = self.custom_tokenizer.tokenize(phrase)\n",
    "        if \"nltk_stoplist\" in self.feature_selection:\n",
    "            #use an NLTK stoplist\n",
    "            if isinstance(phrase, list):#check words if already split into list\n",
    "                #TODO: quicker way to run this?\n",
    "                phrases = []\n",
    "                for partial_phrase in phrase:\n",
    "                    if partial_phrase not in self.NLTK_ENGLISH_STOP:\n",
    "                        phrases.append(partial_phrase)\n",
    "                phrase = phrases\n",
    "            else:#split into list then eval words\n",
    "                phrase = [word for word in phrase.split(\" \") if word not in self.NLTK_ENGLISH_STOP]\n",
    "        if \"spacy_stoplist\" in self.feature_selection:\n",
    "            #use a spacy stoplist\n",
    "            if isinstance(phrase, list):\n",
    "                phrases = []\n",
    "                for partial_phrase in phrase:\n",
    "                    phrases +=  [word for word in partial_phrase.split(\" \") if word not in list(SPACY_STOP)]\n",
    "                phrase = list(itertools.chain.from_iterable(phrases))\n",
    "            else:\n",
    "                phrase = [word for word in phrase.split(\" \") if word not in list(SPACY_STOP)]\n",
    "        if \"nltk_pos_tag\" in self.feature_selection:\n",
    "            #use POS tagging - must be a list, so split by spaces if not already\n",
    "            if isinstance(phrase, list):\n",
    "                phrase = nltk.pos_tag(phrase)\n",
    "            else:\n",
    "                phrase = phrase.split(\" \")\n",
    "                phrase = nltk.pos_tag(phrase)\n",
    "        if \"negation_bigrams\" in self.feature_selection:\n",
    "            #create bigrams with negation words + their successors\n",
    "            if len(self.feature_selection) > 1:\n",
    "                phrases = []\n",
    "                for j in range(0, len(phrase)):\n",
    "                    partial_phrase = phrase[j]\n",
    "                    split = partial_phrase.split(\" \")\n",
    "\n",
    "                    for i in range(0, len(split)):\n",
    "                        if split[i] in negation_words and i != len(split) - 1:\n",
    "                            #add the negation word and its successor to the list of phrases\n",
    "                            phrases.append(split[i] + \" \" + split[i+1])\n",
    "                        else:\n",
    "                            #skip to next partial if negation is at end of partial phrase\n",
    "                            if split[i] in negation_words and j != len(phrase) - 1:\n",
    "                                # print(\"Next partial\")\n",
    "                                phrases.append(split[i] + \" \" + phrase[j+1].split(\" \")[0])\n",
    "                            else:\n",
    "                                #skip entirely if this is the last partial phrase\n",
    "                                phrases.append(split[i])\n",
    "                phrase = phrases\n",
    "            else:\n",
    "                split = phrase.split(\" \")\n",
    "                phrase = []\n",
    "                for i in range(0, len(split)):\n",
    "                    if split[i] in negation_words and i != len(split) - 1:\n",
    "                        phrase.append(split[i] + \" \" + split[i+1])\n",
    "                    else:\n",
    "                        phrase.append(split[i])\n",
    "        if \"intensifier_bigrams\" in self.feature_selection:\n",
    "            #create bigrams with intensifier words + their successors\n",
    "            if len(self.feature_selection) > 1:\n",
    "                phrases = []\n",
    "                for j in range(0, len(phrase)):\n",
    "                    partial_phrase = phrase[j]\n",
    "                    split = partial_phrase.split(\" \")\n",
    "\n",
    "                    for i in range(0, len(split)):\n",
    "                        if split[i] in intensifer_words and i != len(split) - 1:\n",
    "                            #add the intensifier word and its successor to the list of phrases\n",
    "                            phrases.append(split[i] + \" \" + split[i+1])\n",
    "                        else:\n",
    "                            #skip to next partial if intensifier is at end of partial phrase\n",
    "                            if split[i] in intensifer_words and  j != len(phrase) - 1:\n",
    "                                phrases.append(split[i] + phrase[j+1].split(\" \")[0])\n",
    "                            else:\n",
    "                                #skip entirely if this is the last partial phrase\n",
    "                                phrases.append(split[i])\n",
    "                phrase = phrases\n",
    "            else:\n",
    "                split = phrase.split(\" \")\n",
    "                phrase = []\n",
    "                for i in range(0, len(split)):\n",
    "                    if split[i] in intensifer_words and i != len(split) - 1:\n",
    "                        phrase.append(split[i] + \" \" + split[i+1])\n",
    "                    else:\n",
    "                        phrase.append(split[i])\n",
    "\n",
    "        if self.retList:\n",
    "            return phrase\n",
    "        else:#rejoin into one text body in case want to do different tokenisation/one-hot later on\n",
    "            return \" \".join(filter(None, phrase))\n",
    "\n",
    "    def process_phrases(self):\n",
    "        \"\"\"extract bodies\"\"\"\n",
    "\n",
    "        self.bodies = self.data['Body']\n",
    "        \n",
    "        #apply preprocessing function to all phrases using list comprehension\n",
    "        self.preprocessed_phrases = [self.preprocess_phrase(phrase) for phrase in self.data['Body']]\n",
    "        self.features = [self.extract_features_from_phrase(phrase) for phrase in self.preprocessed_phrases]\n",
    "\n",
    "        data = {'id': self.data['id'], 'ppBody': self.features, 'label': self.data['Label']}\n",
    "        self.pp_df = pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a new dataset\n",
    "#run the preprocessing with lowercasing, removing newlines, stemming, and tokenisation using the nltk regex\n",
    "d = Dataset(dfSpam, preprocessing=[\"lower\",\"newlines\",\"stemming\"], feature_selection=[\"nltk_tokenize\"], retList=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the dataframe\n",
    "df = d.pp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1801.0     subject fwd get it all vlco din x anax vl gra ...\n",
       "511.0      yes we do purchase uncollected judicial judgem...\n",
       "12393.0    subject get a costco gold membership this is o...\n",
       "2514.0     on tue sep 17 2002 at 09 11 01pm 0700 paul pre...\n",
       "1279.0     subject re your financial security high priori...\n",
       "2453.0     subject scil 10 call for papers student confer...\n",
       "22488.0    subject hpl 75 th anniversary celebration hous...\n",
       "3687.0     subject fwd vlco din valium vlagr xianax som a...\n",
       "3350.0     subject your domain name flippant house need a...\n",
       "4747.0     subject no more paying for movies events on ca...\n",
       "Name: ppBody, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10, random_state=2)['ppBody']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the spam emails - there are 7328\n",
    "positive = df[df['label'] == \"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get an equal number of non-spam\n",
    "negative = df[df['label'] == \"0\"].sample(7328, random_state=myseed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subject', 'fwd', 'get', 'it', 'all']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"subject fwd get it all\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_doc_and_count(doc):\n",
    "    return Counter(doc.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_word_counters = positive['ppBody'].apply(split_doc_and_count).to_frame()\n",
    "neg_word_counters = negative['ppBody'].apply(split_doc_and_count).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ppBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>{'save': 3, 'up': 2, 'to': 6, '70': 2, 'on': 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>{'1': 1, 'fight': 1, 'the': 4, 'risk': 1, 'of'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>{'1': 1, 'fight': 1, 'the': 4, 'risk': 1, 'of'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>{'adult': 5, 'club': 1, 'offers': 2, 'free': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>{'i': 2, 'thought': 1, 'you': 3, 'might': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429.0</th>\n",
       "      <td>{'subject': 1, 'lucky': 2, 'you': 5, 'congratu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430.0</th>\n",
       "      <td>{'subject': 1, 'new': 8, 'on': 13, 'capitalfm'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431.0</th>\n",
       "      <td>{'subject': 1, 'submit': 4, '600': 4, 'this': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432.0</th>\n",
       "      <td>{'subject': 1, 'submit': 4, '600': 4, 'this': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433.0</th>\n",
       "      <td>{'subject': 1, 'i': 45, 'can': 17, 't': 8, 'st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7328 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  ppBody\n",
       "1.0    {'save': 3, 'up': 2, 'to': 6, '70': 2, 'on': 3...\n",
       "2.0    {'1': 1, 'fight': 1, 'the': 4, 'risk': 1, 'of'...\n",
       "3.0    {'1': 1, 'fight': 1, 'the': 4, 'risk': 1, 'of'...\n",
       "4.0    {'adult': 5, 'club': 1, 'offers': 2, 'free': 1...\n",
       "5.0    {'i': 2, 'thought': 1, 'you': 3, 'might': 1, '...\n",
       "...                                                  ...\n",
       "429.0  {'subject': 1, 'lucky': 2, 'you': 5, 'congratu...\n",
       "430.0  {'subject': 1, 'new': 8, 'on': 13, 'capitalfm'...\n",
       "431.0  {'subject': 1, 'submit': 4, '600': 4, 'this': ...\n",
       "432.0  {'subject': 1, 'submit': 4, '600': 4, 'this': ...\n",
       "433.0  {'subject': 1, 'i': 45, 'can': 17, 't': 8, 'st...\n",
       "\n",
       "[7328 rows x 1 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_word_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0      {'save': 3, 'up': 2, 'to': 6, '70': 2, 'on': 3...\n",
       "2.0      {'1': 1, 'fight': 1, 'the': 4, 'risk': 1, 'of'...\n",
       "3.0      {'1': 1, 'fight': 1, 'the': 4, 'risk': 1, 'of'...\n",
       "4.0      {'adult': 5, 'club': 1, 'offers': 2, 'free': 1...\n",
       "5.0      {'i': 2, 'thought': 1, 'you': 3, 'might': 1, '...\n",
       "                               ...                        \n",
       "429.0    {'subject': 1, 'lucky': 2, 'you': 5, 'congratu...\n",
       "430.0    {'subject': 1, 'new': 8, 'on': 13, 'capitalfm'...\n",
       "431.0    {'subject': 1, 'submit': 4, '600': 4, 'this': ...\n",
       "432.0    {'subject': 1, 'submit': 4, '600': 4, 'this': ...\n",
       "433.0    {'subject': 1, 'i': 45, 'can': 17, 't': 8, 'st...\n",
       "Name: ppBody, Length: 7328, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_word_counters['ppBody']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_word_counter = pos_word_counters['ppBody'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'save': 1426,\n",
       "         'up': 2829,\n",
       "         'to': 46435,\n",
       "         '70': 508,\n",
       "         'on': 10446,\n",
       "         'life': 1189,\n",
       "         'insurance': 376,\n",
       "         'why': 657,\n",
       "         'spend': 151,\n",
       "         'more': 4565,\n",
       "         'than': 1629,\n",
       "         'you': 31865,\n",
       "         'have': 8289,\n",
       "         'quote': 255,\n",
       "         'savings': 218,\n",
       "         'ensuring': 33,\n",
       "         'your': 19707,\n",
       "         'family': 933,\n",
       "         's': 8771,\n",
       "         'financial': 828,\n",
       "         'security': 1092,\n",
       "         'is': 17494,\n",
       "         'very': 1344,\n",
       "         'important': 457,\n",
       "         'makes': 198,\n",
       "         'buying': 134,\n",
       "         'simple': 671,\n",
       "         'and': 38993,\n",
       "         'affordable': 172,\n",
       "         'we': 9479,\n",
       "         'provide': 743,\n",
       "         'free': 4512,\n",
       "         'access': 708,\n",
       "         'the': 57407,\n",
       "         'best': 1850,\n",
       "         'companies': 1192,\n",
       "         'lowest': 303,\n",
       "         'rates': 567,\n",
       "         'fast': 499,\n",
       "         'easy': 1063,\n",
       "         'saves': 16,\n",
       "         'money': 3929,\n",
       "         'let': 709,\n",
       "         'us': 3324,\n",
       "         'help': 1165,\n",
       "         'get': 3915,\n",
       "         'started': 371,\n",
       "         'with': 10670,\n",
       "         'values': 14,\n",
       "         'in': 22340,\n",
       "         'country': 684,\n",
       "         'new': 3085,\n",
       "         'coverage': 76,\n",
       "         'can': 5832,\n",
       "         'hundreds': 411,\n",
       "         'or': 9963,\n",
       "         'even': 1196,\n",
       "         'thousands': 598,\n",
       "         'of': 34897,\n",
       "         'dollars': 990,\n",
       "         'by': 6201,\n",
       "         'requesting': 76,\n",
       "         'a': 28076,\n",
       "         'from': 8508,\n",
       "         'lifequote': 1,\n",
       "         'our': 8018,\n",
       "         'service': 1348,\n",
       "         'will': 8334,\n",
       "         'take': 1346,\n",
       "         'less': 590,\n",
       "         '5': 3628,\n",
       "         'minutes': 409,\n",
       "         'complete': 676,\n",
       "         'shop': 266,\n",
       "         'compare': 180,\n",
       "         'all': 6617,\n",
       "         'types': 82,\n",
       "         'click': 3111,\n",
       "         'here': 4405,\n",
       "         'for': 19706,\n",
       "         'protecting': 22,\n",
       "         'investment': 1481,\n",
       "         'll': 1024,\n",
       "         'ever': 735,\n",
       "         'make': 2749,\n",
       "         'if': 5732,\n",
       "         'are': 9505,\n",
       "         'receipt': 182,\n",
       "         'this': 17870,\n",
       "         'email': 5353,\n",
       "         'error': 373,\n",
       "         'wish': 955,\n",
       "         'be': 9923,\n",
       "         'removed': 926,\n",
       "         'list': 2444,\n",
       "         'please': 3950,\n",
       "         'type': 531,\n",
       "         'remove': 1427,\n",
       "         'reside': 6,\n",
       "         'any': 3740,\n",
       "         'state': 842,\n",
       "         'which': 2094,\n",
       "         'prohibits': 10,\n",
       "         'e': 5095,\n",
       "         'mail': 3952,\n",
       "         'solicitations': 5,\n",
       "         'disregard': 17,\n",
       "         '1': 5574,\n",
       "         'fight': 59,\n",
       "         'risk': 699,\n",
       "         'cancer': 81,\n",
       "         'http': 4930,\n",
       "         'www': 3088,\n",
       "         'adclick': 51,\n",
       "         'ws': 109,\n",
       "         'p': 1294,\n",
       "         'cfm': 61,\n",
       "         'o': 1958,\n",
       "         '315': 43,\n",
       "         'pk0072': 8,\n",
       "         'slim': 24,\n",
       "         'down': 671,\n",
       "         'guaranteed': 535,\n",
       "         'lose': 572,\n",
       "         '10': 2275,\n",
       "         '12': 833,\n",
       "         'lbs': 27,\n",
       "         '30': 1147,\n",
       "         'days': 1236,\n",
       "         '249': 101,\n",
       "         'pk0073': 8,\n",
       "         'child': 118,\n",
       "         'support': 523,\n",
       "         'deserve': 63,\n",
       "         'legal': 617,\n",
       "         'advice': 713,\n",
       "         '245': 44,\n",
       "         'pk0024': 8,\n",
       "         'join': 327,\n",
       "         'web': 1481,\n",
       "         'fastest': 143,\n",
       "         'growing': 257,\n",
       "         'singles': 26,\n",
       "         'community': 161,\n",
       "         '259': 31,\n",
       "         'pk0075': 8,\n",
       "         'start': 997,\n",
       "         'private': 570,\n",
       "         'photo': 83,\n",
       "         'album': 55,\n",
       "         'online': 1612,\n",
       "         '283': 11,\n",
       "         'pk007have': 8,\n",
       "         'wonderful': 82,\n",
       "         'day': 1557,\n",
       "         'offer': 1541,\n",
       "         'manager': 287,\n",
       "         'prizemamaif': 4,\n",
       "         'leave': 136,\n",
       "         'use': 1728,\n",
       "         'link': 1010,\n",
       "         'below': 1237,\n",
       "         'qves': 9,\n",
       "         'com': 6140,\n",
       "         'trim': 20,\n",
       "         'ilug': 144,\n",
       "         'linux': 416,\n",
       "         'ie': 279,\n",
       "         '7c17': 8,\n",
       "         '7c114258': 3,\n",
       "         'irish': 91,\n",
       "         'users': 308,\n",
       "         'group': 495,\n",
       "         'mailman': 200,\n",
       "         'listinfo': 342,\n",
       "         'un': 300,\n",
       "         'subscription': 195,\n",
       "         'information': 3881,\n",
       "         'maintainer': 87,\n",
       "         'listmaster': 87,\n",
       "         'i': 12005,\n",
       "         'zzzz': 34,\n",
       "         'spamassassin': 492,\n",
       "         'taint': 33,\n",
       "         'org': 365,\n",
       "         '7c308417': 5,\n",
       "         'adult': 303,\n",
       "         'club': 130,\n",
       "         'offers': 792,\n",
       "         'membership': 373,\n",
       "         'instant': 264,\n",
       "         'sites': 633,\n",
       "         'now': 3375,\n",
       "         'user': 304,\n",
       "         'name': 2274,\n",
       "         'password': 108,\n",
       "         '7603825': 1,\n",
       "         'internet': 2077,\n",
       "         'news': 1154,\n",
       "         '08': 115,\n",
       "         '18': 476,\n",
       "         '02': 148,\n",
       "         'just': 2809,\n",
       "         'over': 2673,\n",
       "         '2': 4520,\n",
       "         '9': 1110,\n",
       "         'million': 1563,\n",
       "         'members': 420,\n",
       "         'that': 11397,\n",
       "         'signed': 174,\n",
       "         'last': 679,\n",
       "         'month': 838,\n",
       "         'there': 2022,\n",
       "         'were': 888,\n",
       "         '721': 6,\n",
       "         '184': 8,\n",
       "         'one': 3670,\n",
       "         'them': 1613,\n",
       "         'yet': 220,\n",
       "         'faqq': 6,\n",
       "         'offering': 324,\n",
       "         'advertisers': 53,\n",
       "         'pay': 810,\n",
       "         'me': 2838,\n",
       "         'ad': 523,\n",
       "         'space': 124,\n",
       "         'so': 2449,\n",
       "         'don': 1163,\n",
       "         't': 4071,\n",
       "         'q': 396,\n",
       "         'it': 10378,\n",
       "         'true': 295,\n",
       "         'my': 4496,\n",
       "         'absolutely': 363,\n",
       "         'never': 835,\n",
       "         'cent': 43,\n",
       "         'do': 4480,\n",
       "         'give': 772,\n",
       "         'account': 1405,\n",
       "         'friends': 318,\n",
       "         'yes': 303,\n",
       "         'as': 8212,\n",
       "         'long': 732,\n",
       "         'they': 2413,\n",
       "         'age': 316,\n",
       "         'sign': 237,\n",
       "         'no': 5164,\n",
       "         'how': 2521,\n",
       "         'following': 805,\n",
       "         'links': 444,\n",
       "         'become': 471,\n",
       "         'member': 360,\n",
       "         'these': 2226,\n",
       "         'multi': 398,\n",
       "         'dollar': 251,\n",
       "         'operations': 152,\n",
       "         'policies': 58,\n",
       "         'rules': 89,\n",
       "         'fill': 405,\n",
       "         'required': 539,\n",
       "         'info': 976,\n",
       "         'won': 380,\n",
       "         'charge': 307,\n",
       "         'pass': 154,\n",
       "         'believe': 577,\n",
       "         'read': 917,\n",
       "         'their': 2012,\n",
       "         'terms': 397,\n",
       "         'conditions': 148,\n",
       "         'adults': 65,\n",
       "         'farm': 78,\n",
       "         '80': 556,\n",
       "         '71': 90,\n",
       "         '66': 151,\n",
       "         '8': 1347,\n",
       "         'aid': 96,\n",
       "         '760382': 5,\n",
       "         'girls': 151,\n",
       "         'animals': 24,\n",
       "         'getting': 410,\n",
       "         'freaky': 13,\n",
       "         'lifetime': 113,\n",
       "         '4': 2505,\n",
       "         'sexy': 46,\n",
       "         'celebes': 12,\n",
       "         'celebst': 5,\n",
       "         'xxx': 104,\n",
       "         'doing': 324,\n",
       "         '3': 4315,\n",
       "         'play': 243,\n",
       "         'house': 282,\n",
       "         'porn': 123,\n",
       "         'mega': 78,\n",
       "         'live': 534,\n",
       "         'feeds': 17,\n",
       "         '60': 716,\n",
       "         'cams': 32,\n",
       "         'asian': 51,\n",
       "         'sex': 430,\n",
       "         'fantasies': 10,\n",
       "         'japanese': 16,\n",
       "         'schoolgirls': 8,\n",
       "         'shows': 122,\n",
       "         'lesbian': 23,\n",
       "         'lace': 9,\n",
       "         'jennifer': 35,\n",
       "         'simpson': 11,\n",
       "         'miami': 52,\n",
       "         'fl': 165,\n",
       "         'has': 3273,\n",
       "         'entertained': 14,\n",
       "         'boyffriend': 4,\n",
       "         'two': 899,\n",
       "         'years': 1051,\n",
       "         'net': 2037,\n",
       "         'joe': 30,\n",
       "         'morgan': 29,\n",
       "         'manhattan': 11,\n",
       "         'ny': 96,\n",
       "         'unbelievable': 40,\n",
       "         'part': 772,\n",
       "         'about': 2509,\n",
       "         're': 1271,\n",
       "         'removal': 353,\n",
       "         'instructions': 541,\n",
       "         'received': 1091,\n",
       "         'advertisement': 427,\n",
       "         'because': 1184,\n",
       "         'opted': 72,\n",
       "         'receive': 1906,\n",
       "         'specials': 140,\n",
       "         'through': 1462,\n",
       "         'affiliated': 115,\n",
       "         'websites': 219,\n",
       "         'not': 7487,\n",
       "         'further': 681,\n",
       "         'emails': 387,\n",
       "         'may': 2533,\n",
       "         'opt': 374,\n",
       "         'out': 3912,\n",
       "         'database': 363,\n",
       "         'optout': 64,\n",
       "         'allow': 336,\n",
       "         '24': 869,\n",
       "         'hours': 867,\n",
       "         'vonolmosatkirekpup': 1,\n",
       "         'thought': 219,\n",
       "         'might': 423,\n",
       "         'like': 2087,\n",
       "         'freeyankee': 3,\n",
       "         'cgi': 226,\n",
       "         'fy2': 3,\n",
       "         'l': 1137,\n",
       "         '822slim12': 1,\n",
       "         '822nic1': 1,\n",
       "         '822ppl1offer': 1,\n",
       "         'daily': 221,\n",
       "         'deals': 153,\n",
       "         'social': 169,\n",
       "         '7c29': 1,\n",
       "         '7c134077': 1,\n",
       "         'events': 537,\n",
       "         'powerhouse': 8,\n",
       "         'gifting': 4,\n",
       "         'program': 1992,\n",
       "         'want': 1893,\n",
       "         'miss': 173,\n",
       "         'founders': 6,\n",
       "         'major': 501,\n",
       "         'players': 88,\n",
       "         'once': 657,\n",
       "         'where': 827,\n",
       "         'invitationexperts': 1,\n",
       "         'calling': 169,\n",
       "         'way': 1244,\n",
       "         'huge': 429,\n",
       "         'cash': 1006,\n",
       "         'flow': 99,\n",
       "         'conceived': 10,\n",
       "         'leverage': 29,\n",
       "         '000': 4127,\n",
       "         'into': 1611,\n",
       "         '50': 1672,\n",
       "         'againthe': 1,\n",
       "         'question': 175,\n",
       "         'either': 244,\n",
       "         'wealthy': 59,\n",
       "         'am': 1441,\n",
       "         'tossing': 4,\n",
       "         'lifeline': 4,\n",
       "         'sake': 45,\n",
       "         'hope': 273,\n",
       "         'grab': 40,\n",
       "         'onto': 86,\n",
       "         'hold': 111,\n",
       "         'tight': 27,\n",
       "         'ride': 27,\n",
       "         'testimonialshear': 2,\n",
       "         'what': 2265,\n",
       "         'average': 223,\n",
       "         'people': 2515,\n",
       "         'first': 1372,\n",
       "         'few': 736,\n",
       "         'ï': 1852,\n",
       "         '½we': 2,\n",
       "         've': 865,\n",
       "         'again': 744,\n",
       "         'al': 322,\n",
       "         '½i': 32,\n",
       "         'm': 1622,\n",
       "         'single': 248,\n",
       "         'mother': 136,\n",
       "         '½': 1216,\n",
       "         'd': 1526,\n",
       "         'was': 3082,\n",
       "         'sure': 569,\n",
       "         'when': 1664,\n",
       "         'sent': 907,\n",
       "         'off': 835,\n",
       "         'pledge': 23,\n",
       "         'but': 2458,\n",
       "         'got': 458,\n",
       "         'back': 1000,\n",
       "         'next': 1195,\n",
       "         'ky': 18,\n",
       "         'didn': 143,\n",
       "         'found': 395,\n",
       "         'myself': 159,\n",
       "         'partner': 317,\n",
       "         'work': 1503,\n",
       "         'think': 536,\n",
       "         'made': 1342,\n",
       "         'right': 1030,\n",
       "         'decision': 147,\n",
       "         'k': 317,\n",
       "         'c': 1138,\n",
       "         'pick': 171,\n",
       "         'gave': 80,\n",
       "         'leads': 187,\n",
       "         'training': 165,\n",
       "         'too': 547,\n",
       "         'j': 261,\n",
       "         'w': 489,\n",
       "         'caannouncing': 1,\n",
       "         'close': 169,\n",
       "         'sales': 853,\n",
       "         'fax': 960,\n",
       "         'blast': 39,\n",
       "         'immediately': 469,\n",
       "         'upon': 333,\n",
       "         'entry': 59,\n",
       "         'wait': 298,\n",
       "         'call': 1124,\n",
       "         '800': 300,\n",
       "         '421': 11,\n",
       "         '6318': 3,\n",
       "         '896': 6,\n",
       "         '6568': 3,\n",
       "         'name__________________________________phone___________________________________________fax_____________________________________email____________________________________________best': 1,\n",
       "         'time': 3373,\n",
       "         'call_________________________time': 1,\n",
       "         'zone________________________________________this': 1,\n",
       "         'message': 1666,\n",
       "         'compliance': 262,\n",
       "         'bill': 287,\n",
       "         'per': 1136,\n",
       "         'section': 664,\n",
       "         '301': 103,\n",
       "         'paragraph': 176,\n",
       "         '1618': 155,\n",
       "         'transmissions': 51,\n",
       "         'sender': 161,\n",
       "         'stopped': 57,\n",
       "         'at': 5667,\n",
       "         'cost': 912,\n",
       "         'sending': 477,\n",
       "         'reply': 956,\n",
       "         'address': 2468,\n",
       "         'word': 251,\n",
       "         'subject': 6630,\n",
       "         'line': 1085,\n",
       "         'errors': 64,\n",
       "         'omissions': 6,\n",
       "         'exceptions': 18,\n",
       "         'excluded': 50,\n",
       "         'spam': 415,\n",
       "         'compiled': 36,\n",
       "         'replicate': 7,\n",
       "         'relative': 40,\n",
       "         'seattle': 13,\n",
       "         'marketing': 1236,\n",
       "         'gigt': 3,\n",
       "         'turbo': 5,\n",
       "         'team': 290,\n",
       "         'sole': 62,\n",
       "         'purpose': 150,\n",
       "         'communications': 212,\n",
       "         'continued': 67,\n",
       "         'inclusion': 21,\n",
       "         'only': 3410,\n",
       "         'gracious': 6,\n",
       "         'permission': 96,\n",
       "         'send': 2390,\n",
       "         'an': 4412,\n",
       "         'tesrewinter': 3,\n",
       "         'yahoo': 455,\n",
       "         'deleted': 84,\n",
       "         'wanted': 213,\n",
       "         '14': 377,\n",
       "         'year': 1143,\n",
       "         'old': 390,\n",
       "         'fortune': 187,\n",
       "         '500': 856,\n",
       "         'company': 3899,\n",
       "         'tremendous': 96,\n",
       "         'rate': 608,\n",
       "         'looking': 1238,\n",
       "         'individuals': 221,\n",
       "         'who': 1808,\n",
       "         'home': 1839,\n",
       "         'opportunity': 760,\n",
       "         'excellent': 161,\n",
       "         'income': 737,\n",
       "         'experience': 438,\n",
       "         'train': 52,\n",
       "         'employed': 51,\n",
       "         'career': 110,\n",
       "         'vast': 51,\n",
       "         'opportunities': 301,\n",
       "         'then': 1312,\n",
       "         'go': 1428,\n",
       "         'basetel': 24,\n",
       "         'wealthnowwe': 6,\n",
       "         'energetic': 14,\n",
       "         'self': 250,\n",
       "         'motivated': 44,\n",
       "         'form': 948,\n",
       "         'employement': 6,\n",
       "         'specialist': 33,\n",
       "         'contact': 1304,\n",
       "         'html': 696,\n",
       "         '4139volw7': 1,\n",
       "         '758dody1425frhm1': 1,\n",
       "         '764smfc8513fcsll40': 1,\n",
       "         'reliaquote': 8,\n",
       "         'â': 9823,\n",
       "         'change': 612,\n",
       "         'protect': 244,\n",
       "         'future': 1533,\n",
       "         'sufficient': 20,\n",
       "         'residence': 36,\n",
       "         'select': 189,\n",
       "         'alabama': 27,\n",
       "         'alaska': 41,\n",
       "         'arizona': 45,\n",
       "         'arkansas': 15,\n",
       "         'california': 142,\n",
       "         'colorado': 38,\n",
       "         'connecticut': 19,\n",
       "         'delaware': 17,\n",
       "         'dist': 14,\n",
       "         'columbia': 32,\n",
       "         'florida': 112,\n",
       "         'georgia': 80,\n",
       "         'hawaii': 46,\n",
       "         'idaho': 26,\n",
       "         'illinois': 40,\n",
       "         'indiana': 36,\n",
       "         'iowa': 21,\n",
       "         'kansas': 20,\n",
       "         'kentucky': 16,\n",
       "         'louisiana': 37,\n",
       "         'maine': 18,\n",
       "         'maryland': 20,\n",
       "         'massachusetts': 21,\n",
       "         'michigan': 18,\n",
       "         'minnesota': 27,\n",
       "         'mississippi': 13,\n",
       "         'missouri': 23,\n",
       "         'montana': 51,\n",
       "         'nebraska': 13,\n",
       "         'nevada': 32,\n",
       "         'hampshire': 20,\n",
       "         'jersey': 42,\n",
       "         'mexico': 45,\n",
       "         'york': 159,\n",
       "         'north': 218,\n",
       "         'carolina': 57,\n",
       "         'dakota': 36,\n",
       "         'ohio': 22,\n",
       "         'oklahoma': 29,\n",
       "         'oregon': 35,\n",
       "         'pennsylvania': 21,\n",
       "         'rhode': 14,\n",
       "         'island': 129,\n",
       "         'south': 352,\n",
       "         'tennessee': 15,\n",
       "         'texas': 96,\n",
       "         'utah': 26,\n",
       "         'vermont': 13,\n",
       "         'virginia': 49,\n",
       "         'washington': 99,\n",
       "         'west': 125,\n",
       "         'wisconsin': 17,\n",
       "         'wyoming': 114,\n",
       "         'date': 547,\n",
       "         'birth': 39,\n",
       "         'mm': 47,\n",
       "         'dd': 18,\n",
       "         'yy': 6,\n",
       "         '01': 296,\n",
       "         '03': 133,\n",
       "         '04': 136,\n",
       "         '05': 185,\n",
       "         '06': 105,\n",
       "         '07': 93,\n",
       "         '09': 142,\n",
       "         '11': 586,\n",
       "         '13': 253,\n",
       "         '15': 871,\n",
       "         '16': 299,\n",
       "         '17': 399,\n",
       "         '19': 569,\n",
       "         '20': 2976,\n",
       "         '21': 530,\n",
       "         '22': 287,\n",
       "         '23': 197,\n",
       "         '25': 951,\n",
       "         '26': 192,\n",
       "         '27': 421,\n",
       "         '28': 179,\n",
       "         '29': 358,\n",
       "         '31': 275,\n",
       "         '32': 234,\n",
       "         '33': 120,\n",
       "         '34': 120,\n",
       "         '35': 297,\n",
       "         '36': 274,\n",
       "         '37': 104,\n",
       "         '38': 73,\n",
       "         '39': 196,\n",
       "         '40': 589,\n",
       "         '41': 98,\n",
       "         '42': 148,\n",
       "         '43': 96,\n",
       "         '44': 115,\n",
       "         '45': 329,\n",
       "         '46': 197,\n",
       "         '47': 104,\n",
       "         '48': 256,\n",
       "         '49': 269,\n",
       "         '51': 80,\n",
       "         '52': 119,\n",
       "         '53': 37,\n",
       "         '54': 120,\n",
       "         '55': 199,\n",
       "         '56': 87,\n",
       "         '57': 194,\n",
       "         '58': 90,\n",
       "         '59': 158,\n",
       "         '61': 110,\n",
       "         '62': 104,\n",
       "         '63': 85,\n",
       "         '64': 118,\n",
       "         '65': 212,\n",
       "         '67': 72,\n",
       "         '68': 85,\n",
       "         '69': 191,\n",
       "         '72': 154,\n",
       "         '73': 44,\n",
       "         '74': 67,\n",
       "         '75': 476,\n",
       "         '76': 80,\n",
       "         '77': 68,\n",
       "         '78': 88,\n",
       "         '79': 111,\n",
       "         '81': 92,\n",
       "         '82': 84,\n",
       "         '83': 69,\n",
       "         '84': 103,\n",
       "         'male': 173,\n",
       "         'female': 72,\n",
       "         'used': 620,\n",
       "         'tobacco': 41,\n",
       "         'products': 1224,\n",
       "         'months': 623,\n",
       "         'amount': 416,\n",
       "         '100': 1902,\n",
       "         '125': 116,\n",
       "         '150': 253,\n",
       "         '175': 47,\n",
       "         '200': 453,\n",
       "         '225': 47,\n",
       "         '250': 285,\n",
       "         '275': 27,\n",
       "         '300': 373,\n",
       "         '325': 19,\n",
       "         '350': 44,\n",
       "         '375': 33,\n",
       "         '400': 239,\n",
       "         '425': 22,\n",
       "         '450': 41,\n",
       "         '475': 12,\n",
       "         '550': 126,\n",
       "         '600': 146,\n",
       "         '650': 24,\n",
       "         '700': 115,\n",
       "         '750': 72,\n",
       "         '850': 50,\n",
       "         '900': 72,\n",
       "         '950': 17,\n",
       "         '6': 1557,\n",
       "         '7': 1733,\n",
       "         'need': 1778,\n",
       "         'instantly': 106,\n",
       "         'quotes': 86,\n",
       "         'highly': 230,\n",
       "         'rated': 57,\n",
       "         'toon': 5,\n",
       "         'today': 1464,\n",
       "         'nothing': 503,\n",
       "         'copyright': 139,\n",
       "         '2001': 174,\n",
       "         'inc': 1196,\n",
       "         'rights': 310,\n",
       "         'reserved': 113,\n",
       "         'receiving': 590,\n",
       "         'mailing': 1126,\n",
       "         'sendgreatoffers': 21,\n",
       "         'subscribed': 59,\n",
       "         'jm': 124,\n",
       "         'netnoteinc': 116,\n",
       "         'unsubscribe': 652,\n",
       "         'admanmail': 18,\n",
       "         'asp': 100,\n",
       "         'em': 190,\n",
       "         'sgo': 17,\n",
       "         'must': 977,\n",
       "         'also': 1737,\n",
       "         'include': 729,\n",
       "         'body': 413,\n",
       "         'unsubscribed': 93,\n",
       "         'correspondence': 129,\n",
       "         'services': 1050,\n",
       "         'should': 1126,\n",
       "         'directed': 55,\n",
       "         'tired': 98,\n",
       "         'bull': 57,\n",
       "         'stop': 719,\n",
       "         'losing': 77,\n",
       "         'real': 767,\n",
       "         'maker': 51,\n",
       "         'experts': 85,\n",
       "         'invitation': 79,\n",
       "         'big': 405,\n",
       "         'boys': 45,\n",
       "         'arethis': 1,\n",
       "         'system': 1052,\n",
       "         'drive': 314,\n",
       "         'doorstep': 23,\n",
       "         'short': 297,\n",
       "         'period': 153,\n",
       "         '1000': 222,\n",
       "         '00': 3480,\n",
       "         'cathis': 1,\n",
       "         'most': 1418,\n",
       "         'announcing': 16,\n",
       "         'print': 244,\n",
       "         'successleads': 1,\n",
       "         'firemail': 1,\n",
       "         'de': 1782,\n",
       "         'telephone': 272,\n",
       "         'number': 1492,\n",
       "         'responding': 25,\n",
       "         '499': 42,\n",
       "         '99': 1116,\n",
       "         'value': 467,\n",
       "         'name___________________________________': 1,\n",
       "         'phone___________________________________': 1,\n",
       "         'fax_____________________________________': 1,\n",
       "         'email___________________________________': 1,\n",
       "         'dear': 545,\n",
       "         'ricardo1': 2,\n",
       "         'effective': 352,\n",
       "         'direct': 407,\n",
       "         'advertising': 491,\n",
       "         'promote': 88,\n",
       "         'business': 3606,\n",
       "         'low': 961,\n",
       "         'addresses': 1203,\n",
       "         'maximize': 18,\n",
       "         '309': 26,\n",
       "         '407': 13,\n",
       "         '7378': 2,\n",
       "         'consultant': 54,\n",
       "         'discuss': 56,\n",
       "         'needs': 301,\n",
       "         '___________________________________________________________________': 6,\n",
       "         '_______________________________________________________________': 7,\n",
       "         '________________________________________________________________': 7,\n",
       "         'city': 410,\n",
       "         '_____________________________________________________________________': 3,\n",
       "         'phone': 1206,\n",
       "         '__________________________________________________________________': 7,\n",
       "         'website': 1095,\n",
       "         '_______________________________________________________': 3,\n",
       "         '___________________________________________________________________________': 12,\n",
       "         'comments': 95,\n",
       "         'details': 527,\n",
       "         'pricing': 99,\n",
       "         'etc': 406,\n",
       "         'market': 1191,\n",
       "         '247': 33,\n",
       "         'po1': 19,\n",
       "         'kj': 30,\n",
       "         '_8j7bjk9': 16,\n",
       "         'h': 502,\n",
       "         'tg0bk5nkiys5': 21,\n",
       "         'cellular': 21,\n",
       "         'accessories': 76,\n",
       "         'wholesale': 104,\n",
       "         'prices': 912,\n",
       "         '202': 26,\n",
       "         '101': 113,\n",
       "         '163': 35,\n",
       "         'merchant': 84,\n",
       "         'hands': 119,\n",
       "         'ear': 15,\n",
       "         'buds': 10,\n",
       "         'holsters': 5,\n",
       "         '98': 241,\n",
       "         'booster': 16,\n",
       "         'antennas': 1,\n",
       "         '0': 2571,\n",
       "         'cases': 69,\n",
       "         'car': 281,\n",
       "         'chargers': 13,\n",
       "         'face': 461,\n",
       "         'plates': 19,\n",
       "         'lithium': 7,\n",
       "         'ion': 117,\n",
       "         'batteries': 12,\n",
       "         '94': 39,\n",
       "         'nokia': 8,\n",
       "         'motorola': 21,\n",
       "         'lg': 24,\n",
       "         'nextel': 6,\n",
       "         'samsung': 14,\n",
       "         'qualcomm': 5,\n",
       "         'ericsson': 10,\n",
       "         'audiovox': 6,\n",
       "         'phones': 70,\n",
       "         'assistance': 386,\n",
       "         '732': 17,\n",
       "         '751': 27,\n",
       "         '1457': 5,\n",
       "         'mailings': 490,\n",
       "         'request': 478,\n",
       "         'removemenow68994': 3,\n",
       "         'btamail': 94,\n",
       "         'cn': 107,\n",
       "         'thank': 734,\n",
       "         'super': 306,\n",
       "         'simply': 671,\n",
       "         'amateur': 62,\n",
       "         'girl': 130,\n",
       "         'door': 174,\n",
       "         'tour': 60,\n",
       "         'photos': 57,\n",
       "         'sneeky': 1,\n",
       "         'hidden': 92,\n",
       "         'nude': 26,\n",
       "         'exibitionists': 1,\n",
       "         'cheating': 30,\n",
       "         'wives': 21,\n",
       "         'girlfriends': 7,\n",
       "         'remov': 20,\n",
       "         'cell': 118,\n",
       "         'voicestream': 3,\n",
       "         'wireless': 165,\n",
       "         'credit': 1327,\n",
       "         'approval': 142,\n",
       "         'activate': 18,\n",
       "         'activation': 12,\n",
       "         'fee': 279,\n",
       "         'applies': 41,\n",
       "         'activations': 1,\n",
       "         'available': 1275,\n",
       "         'areas': 80,\n",
       "         'fulfilled': 4,\n",
       "         'simplywireless': 1,\n",
       "         'authorized': 33,\n",
       "         'dealer': 183,\n",
       "         'see': 1558,\n",
       "         'site': 1674,\n",
       "         'additional': 439,\n",
       "         'rebate': 7,\n",
       "         'plans': 323,\n",
       "         'greater': 86,\n",
       "         'ends': 37,\n",
       "         'conferencing': 28,\n",
       "         'cents': 150,\n",
       "         'minute': 250,\n",
       "         'including': 706,\n",
       "         'distance': 91,\n",
       "         'setup': 74,\n",
       "         'fees': 201,\n",
       "         'contracts': 173,\n",
       "         'monthly': 253,\n",
       "         'anytime': 139,\n",
       "         'anywhere': 310,\n",
       "         'connects': 20,\n",
       "         'participants': 199,\n",
       "         'simplicity': 22,\n",
       "         'set': 475,\n",
       "         'administration': 96,\n",
       "         'operator': 35,\n",
       "         'highest': 117,\n",
       "         'quality': 660,\n",
       "         'industry': 597,\n",
       "         'find': 1155,\n",
       "         'lower': 178,\n",
       "         'every': 1247,\n",
       "         'input': 56,\n",
       "         'field': 181,\n",
       "         'businessto': 7,\n",
       "         'distribution': 180,\n",
       "         'lists': 882,\n",
       "         'watch': 557,\n",
       "         'sporting': 16,\n",
       "         'movies': 166,\n",
       "         'view': 373,\n",
       "         'assemble': 18,\n",
       "         'electronic': 184,\n",
       "         'store': 278,\n",
       "         'parts': 71,\n",
       "         'z': 136,\n",
       "         'follow': 621,\n",
       "         'assembly': 24,\n",
       "         'original': 329,\n",
       "         'drawings': 7,\n",
       "         'plus': 513,\n",
       "         'something': 298,\n",
       "         'without': 1075,\n",
       "         'report': 3130,\n",
       "         'using': 734,\n",
       "         'descrambler': 52,\n",
       "         'legally': 98,\n",
       "         'warning': 61,\n",
       "         'build': 330,\n",
       "         'tv': 237,\n",
       "         'reading': 198,\n",
       "         'frequently': 23,\n",
       "         'asked': 159,\n",
       "         'questions': 441,\n",
       "         'cable': 199,\n",
       "         'fiber': 16,\n",
       "         'tci': 15,\n",
       "         'jarrod': 6,\n",
       "         'answer': 279,\n",
       "         'converter': 11,\n",
       "         'box': 535,\n",
       "         'plan': 378,\n",
       "         'works': 603,\n",
       "         'specific': 188,\n",
       "         'included': 282,\n",
       "         'each': 1418,\n",
       "         'detect': 19,\n",
       "         'signal': 27,\n",
       "         'descrambles': 5,\n",
       "         'does': 1035,\n",
       "         'move': 453,\n",
       "         'alter': 97,\n",
       "         'existing': 185,\n",
       "         'television': 98,\n",
       "         'vcr': 14,\n",
       "         'remote': 96,\n",
       "         'control': 441,\n",
       "         'manually': 9,\n",
       "         'controlled': 23,\n",
       "         'comes': 213,\n",
       "         'picture': 97,\n",
       "         'guide': 523,\n",
       "         'everywhere': 20,\n",
       "         'across': 115,\n",
       "         'usa': 365,\n",
       "         'england': 67,\n",
       "         'brazil': 23,\n",
       "         'canada': 278,\n",
       "         'other': 1884,\n",
       "         'countries': 248,\n",
       "         'deal': 322,\n",
       "         'unhappy': 28,\n",
       "         'reason': 291,\n",
       "         'refund': 89,\n",
       "         'order': 2755,\n",
       "         'act': 969,\n",
       "         'within': 2089,\n",
       "         'bonus': 290,\n",
       "         'manual': 155,\n",
       "         ...})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_word_counter = neg_word_counters['ppBody'].sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Run some tests on the datasets from `Email-Spam-Dataset`:\n",
    "- find most common words in spam/not spam emails - plot on histogram\n",
    "- classify the sentiment of each email as positive/negative and see if there's a pattern in spam/not spam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF\n",
    "\n",
    "Work in Progress: running TFIDF to classify Email-Spam-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english', max_features=10000)\n",
    "features = tfidf.fit_transform(df.ppBody).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18651, 10000)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = d.pp_df.iloc[:, 0:2]\n",
    "y = d.pp_df.iloc[:, 2]\n",
    "\n",
    "ss = ShuffleSplit(n_splits=1, test_size=0.25, random_state=10)\n",
    "indexes = list(ss.split(X, y))\n",
    "train_set  = indexes[0][0]\n",
    "test_set  = indexes[0][1]\n",
    "Xtrain = X.iloc[train_set, :]\n",
    "ytrain = y.iloc[train_set]\n",
    "Xtest = X.iloc[test_set, :]\n",
    "ytest = y.iloc[test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Classify with word2vec (see https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "Classify with BERT (also using TDS article methods)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Use RF/ensemble method for some of the classifiers above to see which features work best and which preprocessing methods are good - exploring Phishing-Dataset-for-Machine-Learning features first may be helpful"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phishing-Dataset-for-Machine-Learning\n",
    "\n",
    "Inspect this dataset - it is for webpages (https://www.sciencedirect.com/science/article/pii/S0020025519300763?via%3Dihub) but some of the features may be applicable to email classification\n",
    "\n",
    "Other repos that use this dataset to take inspiration from:\n",
    "- https://github.com/andpalmier/MLWithPhishing\n",
    "- https://github.com/rewanthtammana/Detect-phishing-websites-using-ML\n",
    "- https://www.kaggle.com/code/fadilparves/pishing-detection-using-machine-learning\n",
    "\n",
    "Random forests used to discover best features - we could apply similar to the Email-Spam-Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Feature Extraction\n",
    "\n",
    "Can we apply the features used in this dataset to Email-Spam-Dataset?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "\n",
    "- [x] Remove NaN id from merged `dfSpam`\n",
    "- [ ] Tokenise body data\n",
    "- [ ] Set seed method\n",
    "- [ ] Stratified Sample?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
