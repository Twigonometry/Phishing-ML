{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phishing Email Generation and Classification\n",
    "\n",
    "Experimental notebook for classification and generation of phishing emails using several techniques.\n",
    "\n",
    "Roadmap:\n",
    "- Preprocessing\n",
    "    - Sentiment Analysis\n",
    "- Classification\n",
    "    - Logistic Regression on emails\n",
    "    - word2vec model\n",
    "    - Transformer-based model\n",
    "- Generation\n",
    "    - With GANs\n",
    "    - With T5\n",
    "    - With transformers\n",
    "- Adversarial Learning\n",
    "    - EA-based method of classifying\n",
    "    - Cyclical GANs\n",
    "    - Use EA to seed an NN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements - uncomment this line the first time you run this notebook\n",
    "#!pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "myseed = 10897"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email-Spam-Dataset\n",
    "\n",
    "Use this dataset as preliminary exploration of classification techniques. Classifier can then be applied to Enron dataset if it works\n",
    "\n",
    "0: not spam\n",
    "1: spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_col_names = ['id', 'Body', 'Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSA = pd.read_csv('./kaggle-datasets/Email-Spam-Dataset/completeSpamAssassin.csv', names=spam_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEnron = pd.read_csv('./kaggle-datasets/Email-Spam-Dataset/enronSpamSubset.csv', names=spam_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLing = pd.read_csv('./kaggle-datasets/Email-Spam-Dataset/lingSpam.csv', names=spam_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge datasets\n",
    "dfs = [dfSA, dfEnron, dfLing]\n",
    "dfSpam = pd.concat(dfs).dropna(axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a peek at the body of the emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>\\nSave up to 70% on Life Insurance.\\nWhy Spend...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>##############################################...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I thought you might like these:\\n1) Slim Down ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               Body Label\n",
       "1.0  0.0  \\nSave up to 70% on Life Insurance.\\nWhy Spend...     1\n",
       "2.0  1.0  1) Fight The Risk of Cancer!\\nhttp://www.adcli...     1\n",
       "3.0  2.0  1) Fight The Risk of Cancer!\\nhttp://www.adcli...     1\n",
       "4.0  3.0  ##############################################...     1\n",
       "5.0  4.0  I thought you might like these:\\n1) Slim Down ...     1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSpam.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In avoidance of 'self-plagiarisation', much of the code for the `Dataset` class is adapted from my Text Processing Sentiment Analysis assignment. Code can be provided to markers on request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords as NLTK_STOP\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download wordnet for lemmatization\n",
    "#uncomment appropriate line if you get error: \"Resource wordnet not found.\", \"Resource punkt not found.\", etc...\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theco\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#spacy imports\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOP\n",
    "\n",
    "#download spacy dataset\n",
    "#uncomment the line below if you get error \"Can't find model 'en_core_web_sm'\"\n",
    "# ! python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset` Class - this takes a `Dataframe` and applies various preprocessing/feature selection techniques to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, df, preprocessing=[\"lower\"], feature_selection=[\"alltokens\"], retList=False):\n",
    "        \n",
    "        #read preprocessing and feature_selection configuration\n",
    "        self.preprocessing = preprocessing\n",
    "        self.feature_selection = feature_selection\n",
    "        self.data = df\n",
    "        self.retList = retList\n",
    "\n",
    "        # === define various processors and regexes for various preprocessing/feature selection methods ===\n",
    "\n",
    "        # NLTK Stemming Engine\n",
    "        self.porter = PorterStemmer()\n",
    "\n",
    "        #NLTK Lemmatizing Engine\n",
    "        self.wn_lt = WordNetLemmatizer()\n",
    "\n",
    "        #tokenizers\n",
    "\n",
    "        #words regex - splits on word boundaries, doesn't include punctuation etc\n",
    "        self.word_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "        #create tokenizer based on NLTK-provided regex from Labs\n",
    "        nltk_pat = r'''(?x) # set flag to allow verbose regexps\n",
    "            (?:[A-Z]\\.)+ # abbreviations, e.g. U.S.A.\n",
    "            | \\w+(?:-\\w+)* # words with optional internal hyphens\n",
    "            | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "            | \\.\\.\\. # ellipsis\n",
    "            | [][.,;\"'?():_`-]\n",
    "            | [>]?[:;][\\']?[\\(\\)\\[\\]]+ # these are separate tokens; includes ], [\n",
    "            '''\n",
    "        self.nltk_tokenizer = RegexpTokenizer(nltk_pat)\n",
    "\n",
    "        #create tokenizer based on custom regex based on above, with less features\n",
    "        custom_pat = r'''(?x)\n",
    "                \\w+(?:-\\w+)*\n",
    "                |\\$?\\d+(?:\\.\\d+)?%?\n",
    "                |\\.\\.\\.'''\n",
    "        self.custom_tokenizer = RegexpTokenizer(custom_pat)\n",
    "\n",
    "        # stop words - SPACY_STOP defined above in imports\n",
    "\n",
    "        self.NLTK_ENGLISH_STOP = set(NLTK_STOP.words('english'))\n",
    "\n",
    "        # === apply preprocessing and feature selection ===\n",
    "\n",
    "        self.process_phrases()\n",
    "    \n",
    "    def preprocess_phrase(self, phrase):\n",
    "        \"\"\"define preprocessing function for phrases\n",
    "        can call any number of these options - however, some may not combine well\"\"\"\n",
    "\n",
    "        if self.preprocessing == []:\n",
    "            #no preprocessing\n",
    "            return phrase\n",
    "        if \"lower\" in self.preprocessing:\n",
    "            #lowercase\n",
    "            phrase = phrase.lower()\n",
    "        if \"newlines\" in self.preprocessing:\n",
    "            phrase = \" \".join(filter(None, phrase.split(\"\\n\")))\n",
    "        if \"punc\" in self.preprocessing:\n",
    "            #remove punctuation\n",
    "            phrase = phrase.translate(str.maketrans('','',string.punctuation))\n",
    "        if \"stemming\" in self.preprocessing:\n",
    "            #use NLTK stemming\n",
    "            phrase = self.porter.stem(phrase)\n",
    "        if \"nltk_lemmatize\" in self.preprocessing:\n",
    "            #use NLTK's lemmatization method\n",
    "            new_phrase = \"\"\n",
    "            words = nltk.word_tokenize(phrase)\n",
    "            for word in words:\n",
    "                new_phrase += self.wn_lt.lemmatize(word)\n",
    "            phrase = new_phrase\n",
    "        if \"spacy_lemmatize\" in self.preprocessing:\n",
    "            #use spacy's lemmatization method\n",
    "            nlp_phrase = nlp(phrase)\n",
    "            new_phrase = \"\"\n",
    "            for token in nlp_phrase:\n",
    "                new_phrase += (token.lemma_ + \" \")\n",
    "            phrase = new_phrase\n",
    "\n",
    "        return phrase\n",
    "\n",
    "    def extract_features_from_phrase(self, phrase):\n",
    "        \"\"\"define feature extraction function for phrases\n",
    "        extracts all words from all phrases as features for document set\n",
    "        for each method, if one has already been applied the phrase must be treated as a list\"\"\"\n",
    "\n",
    "        #list of negation words from \n",
    "        negation_words = [\"neither\", \"never\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"nowhere\"]\n",
    "\n",
    "        intensifer_words = [\"absolutely\", \"completely\", \"extremely\", \"highly\", \"rather\", \"really\", \"so\", \"too\", \"totally\", \"utterly\", \"very\"]\n",
    "\n",
    "        #use if len(self.feature_selection) > 1 to check if any preprocessing already occurred, as input will be in a list\n",
    "\n",
    "        if self.feature_selection == []:\n",
    "            #we have to do some feature selection, absolute minimum is alltokens\n",
    "            return phrase.split(\" \")\n",
    "        if \"alltokens\" in self.feature_selection:\n",
    "            #split based on whitespace\n",
    "            phrase = phrase.split(\" \")\n",
    "        if \"nltk_tokenize\" in self.feature_selection:\n",
    "            #tokenize using NLTK tokenizer, with words regex\n",
    "            if isinstance(phrase, list):\n",
    "                phrase =  list(itertools.chain.from_iterable([self.word_tokenizer.tokenize(partial_phrase) for partial_phrase in phrase]))\n",
    "            else:\n",
    "                phrase = self.word_tokenizer.tokenize(phrase)\n",
    "                # print(phrase)\n",
    "        if \"nltk_tokenize_2\" in self.feature_selection:\n",
    "            #tokenize using NLTK's regex\n",
    "            if isinstance(phrase, list):\n",
    "                phrase =  list(itertools.chain.from_iterable([self.nltk_tokenizer.tokenize(partial_phrase) for partial_phrase in phrase]))\n",
    "            else:\n",
    "                phrase = self.nltk_tokenizer.tokenize(phrase)\n",
    "        if \"custom_tokenize\" in self.feature_selection:\n",
    "            #tokenize with custom regex\n",
    "            if isinstance(phrase, list):\n",
    "                phrase = list(itertools.chain.from_iterable([self.custom_tokenizer.tokenize(partial_phrase) for partial_phrase in phrase]))\n",
    "            else:\n",
    "                phrase = self.custom_tokenizer.tokenize(phrase)\n",
    "        if \"nltk_stoplist\" in self.feature_selection:\n",
    "            #use an NLTK stoplist\n",
    "            if isinstance(phrase, list):#check words if already split into list\n",
    "                #TODO: quicker way to run this?\n",
    "                phrases = []\n",
    "                for partial_phrase in phrase:\n",
    "                    if partial_phrase not in self.NLTK_ENGLISH_STOP:\n",
    "                        phrases.append(partial_phrase)\n",
    "                phrase = phrases\n",
    "            else:#split into list then eval words\n",
    "                phrase = [word for word in phrase.split(\" \") if word not in self.NLTK_ENGLISH_STOP]\n",
    "        if \"spacy_stoplist\" in self.feature_selection:\n",
    "            #use a spacy stoplist\n",
    "            if isinstance(phrase, list):\n",
    "                phrases = []\n",
    "                for partial_phrase in phrase:\n",
    "                    phrases +=  [word for word in partial_phrase.split(\" \") if word not in list(SPACY_STOP)]\n",
    "                phrase = list(itertools.chain.from_iterable(phrases))\n",
    "            else:\n",
    "                phrase = [word for word in phrase.split(\" \") if word not in list(SPACY_STOP)]\n",
    "        if \"nltk_pos_tag\" in self.feature_selection:\n",
    "            #use POS tagging - must be a list, so split by spaces if not already\n",
    "            if isinstance(phrase, list):\n",
    "                phrase = nltk.pos_tag(phrase)\n",
    "            else:\n",
    "                phrase = phrase.split(\" \")\n",
    "                phrase = nltk.pos_tag(phrase)\n",
    "        if \"negation_bigrams\" in self.feature_selection:\n",
    "            #create bigrams with negation words + their successors\n",
    "            if len(self.feature_selection) > 1:\n",
    "                phrases = []\n",
    "                for j in range(0, len(phrase)):\n",
    "                    partial_phrase = phrase[j]\n",
    "                    split = partial_phrase.split(\" \")\n",
    "\n",
    "                    for i in range(0, len(split)):\n",
    "                        if split[i] in negation_words and i != len(split) - 1:\n",
    "                            #add the negation word and its successor to the list of phrases\n",
    "                            phrases.append(split[i] + \" \" + split[i+1])\n",
    "                        else:\n",
    "                            #skip to next partial if negation is at end of partial phrase\n",
    "                            if split[i] in negation_words and j != len(phrase) - 1:\n",
    "                                # print(\"Next partial\")\n",
    "                                phrases.append(split[i] + \" \" + phrase[j+1].split(\" \")[0])\n",
    "                            else:\n",
    "                                #skip entirely if this is the last partial phrase\n",
    "                                phrases.append(split[i])\n",
    "                phrase = phrases\n",
    "            else:\n",
    "                split = phrase.split(\" \")\n",
    "                phrase = []\n",
    "                for i in range(0, len(split)):\n",
    "                    if split[i] in negation_words and i != len(split) - 1:\n",
    "                        phrase.append(split[i] + \" \" + split[i+1])\n",
    "                    else:\n",
    "                        phrase.append(split[i])\n",
    "        if \"intensifier_bigrams\" in self.feature_selection:\n",
    "            #create bigrams with intensifier words + their successors\n",
    "            if len(self.feature_selection) > 1:\n",
    "                phrases = []\n",
    "                for j in range(0, len(phrase)):\n",
    "                    partial_phrase = phrase[j]\n",
    "                    split = partial_phrase.split(\" \")\n",
    "\n",
    "                    for i in range(0, len(split)):\n",
    "                        if split[i] in intensifer_words and i != len(split) - 1:\n",
    "                            #add the intensifier word and its successor to the list of phrases\n",
    "                            phrases.append(split[i] + \" \" + split[i+1])\n",
    "                        else:\n",
    "                            #skip to next partial if intensifier is at end of partial phrase\n",
    "                            if split[i] in intensifer_words and  j != len(phrase) - 1:\n",
    "                                phrases.append(split[i] + phrase[j+1].split(\" \")[0])\n",
    "                            else:\n",
    "                                #skip entirely if this is the last partial phrase\n",
    "                                phrases.append(split[i])\n",
    "                phrase = phrases\n",
    "            else:\n",
    "                split = phrase.split(\" \")\n",
    "                phrase = []\n",
    "                for i in range(0, len(split)):\n",
    "                    if split[i] in intensifer_words and i != len(split) - 1:\n",
    "                        phrase.append(split[i] + \" \" + split[i+1])\n",
    "                    else:\n",
    "                        phrase.append(split[i])\n",
    "\n",
    "        if self.retList:\n",
    "            return phrase\n",
    "        else:#rejoin into one text body in case want to do different tokenisation/one-hot later on\n",
    "            return \" \".join(filter(None, phrase))\n",
    "\n",
    "    def process_phrases(self):\n",
    "        \"\"\"extract bodies\"\"\"\n",
    "\n",
    "        self.bodies = self.data['Body']\n",
    "        \n",
    "        #apply preprocessing function to all phrases using list comprehension\n",
    "        self.preprocessed_phrases = [self.preprocess_phrase(phrase) for phrase in self.data['Body']]\n",
    "        self.features = [self.extract_features_from_phrase(phrase) for phrase in self.preprocessed_phrases]\n",
    "\n",
    "        data = {'id': self.data['id'], 'ppBody': self.features, 'label': self.data['Label']}\n",
    "        self.pp_df = pd.DataFrame(data=data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate our dataset `d` with basic feature selection + tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a new dataset\n",
    "#run the preprocessing with lowercasing, removing newlines, stemming, and tokenisation using the nltk regex\n",
    "d = Dataset(dfSpam, preprocessing=[\"lower\",\"newlines\",\"stemming\"], feature_selection=[\"nltk_tokenize\"], retList=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['split', 'me']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"split me\"\n",
    "a.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_doc_and_count(doc, delim=' '):\n",
    "    \"\"\"this splits a document (split by spaces by default)\n",
    "    then runs Counter on the result\"\"\"\n",
    "    return Counter(doc.split(delim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset):\n",
    "    \"\"\"applies post-processing to the dataframe rows\"\"\"\n",
    "\n",
    "    #load the text body\n",
    "    df = dataset.pp_df\n",
    "\n",
    "    #find the negative and positive classes\n",
    "    positive = df[df['label'] == \"1\"]\n",
    "    negative = df[df['label'] == \"0\"]\n",
    "\n",
    "    l_positive = len(positive)\n",
    "    l_negative = len(negative)\n",
    "\n",
    "    #which class is bigger?\n",
    "    min_len = min(l_positive, l_negative)\n",
    "\n",
    "    #take a random sample of the smaller class to remove class imbalance\n",
    "    if l_positive > l_negative:\n",
    "        positive = positive.sample(min_len, random_state=myseed)\n",
    "    elif l_negative > l_positive:\n",
    "        negative = negative.sample(min_len, random_state=myseed)\n",
    "\n",
    "    #create counters from the text bodies\n",
    "    dataset.pos_word_counter = positive['ppBody'].apply(split_doc_and_count).sum()\n",
    "    dataset.neg_word_counter = negative['ppBody'].apply(split_doc_and_count).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the below if you are not loading the counts from a pickle file\n",
    "# process_dataset(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = Dataset(dfSpam, preprocessing=[\"lower\",\"newlines\",\"stemming\"], feature_selection=[\"nltk_tokenize\", \"nltk_stoplist\"], retList=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the below if you are not loading the counts from a pickle file\n",
    "# process_dataset(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pickle(object, path):\n",
    "    if not os.path.exists('./Pickle'):\n",
    "        os.makedirs('./Pickle')\n",
    "\n",
    "    with open('./Pickle/' + path, 'wb') as output:\n",
    "        pickle.dump(object, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(path):\n",
    "    with open('./Pickle/' + path, 'rb') as input:\n",
    "        return pickle.load(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the below to save your counters to a file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_pickle(d.pos_word_counter, 'd-pos.pickle')\n",
    "# save_pickle(d.neg_word_counter, 'd-neg.pickle')\n",
    "# save_pickle(d2.pos_word_counter, 'd-pos.pickle')\n",
    "# save_pickle(d2.neg_word_counter, 'd-neg.pickle')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the below to load your saved counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_pos_counts = load_pickle('d-pos.pickle')\n",
    "d1_neg_counts = load_pickle('d-neg.pickle')\n",
    "d2_pos_counts = load_pickle('d2-pos.pickle')\n",
    "d2_neg_counts = load_pickle('d2-neg.pickle')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Run some tests on the datasets from `Email-Spam-Dataset`:\n",
    "- find most common words in spam/not spam emails - plot on histogram\n",
    "- classify the sentiment of each email as positive/negative and see if there's a pattern in spam/not spam"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Words\n",
    "\n",
    "Pure count-based word analysis - better measure of informativeness can be achieved using TFIDF, but this also helps us identify what \"useless\" tokens we may have failed to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_most_common_n(counter, n, name):\n",
    "    print(f\"{n} most common tokens in {name}:\\n\")\n",
    "\n",
    "    most_common = counter.most_common(n)\n",
    "\n",
    "    if n < 6:\n",
    "        output = \", \".join([f\"\\\"{word}\\\" ({count})\" for (word, count) in most_common])\n",
    "    else:\n",
    "        output = \"\\n\".join([f\"{word} ({count})\" for (word, count) in most_common])\n",
    "\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 most common tokens in dfSpam (with stopwords) - Spam emails:\n",
      "\n",
      "\"the\" (57403), \"_\" (52195), \"to\" (46435), \"and\" (38993), \"of\" (34897)\n"
     ]
    }
   ],
   "source": [
    "print_most_common_n(d1_pos_counts, 5, \"dfSpam (with stopwords) - Spam emails\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 most common tokens in dfSpam (stopwords removed) - non-Spam emails:\n",
      "\n",
      "_ (174714)\n",
      "language (15162)\n",
      "university (15135)\n",
      "subject (14113)\n",
      "1 (13936)\n",
      "http (12953)\n",
      "enron (12349)\n",
      "com (11705)\n",
      "e (11580)\n",
      "2 (10140)\n",
      "one (9584)\n",
      "0 (9432)\n",
      "00 (9220)\n",
      "information (9021)\n",
      "mail (8764)\n",
      "www (8569)\n",
      "would (8195)\n",
      "10 (8140)\n",
      "new (8067)\n",
      "please (8060)\n"
     ]
    }
   ],
   "source": [
    "print_most_common_n(d2_neg_counts, 20, \"dfSpam (stopwords removed) - non-Spam emails\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(counter, n, name):\n",
    "    words, counts = list(zip(*counter.most_common(n)))\n",
    "    plt.hist(counts, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConversionError",
     "evalue": "Failed to convert value(s) to axis units: ('_', 'language', 'university', 'subject', '1', 'http', 'enron', 'com', 'e', '2')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\matplotlib\\axis.py:1665\u001b[0m, in \u001b[0;36mAxis.convert_units\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1664\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1665\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconverter\u001b[39m.\u001b[39;49mconvert(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munits, \u001b[39mself\u001b[39;49m)\n\u001b[0;32m   1666\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\matplotlib\\category.py:49\u001b[0m, in \u001b[0;36mStrCategoryConverter.convert\u001b[1;34m(value, unit, axis)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mif\u001b[39;00m unit \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     50\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mMissing category information for StrCategoryConverter; \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     51\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mthis might be caused by unintendedly mixing categorical and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     52\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mnumeric data\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     53\u001b[0m StrCategoryConverter\u001b[39m.\u001b[39m_validate_unit(unit)\n",
      "\u001b[1;31mValueError\u001b[0m: Missing category information for StrCategoryConverter; this might be caused by unintendedly mixing categorical and numeric data",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mConversionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\theco\\Documents\\Phishing-ML\\classif.ipynb Cell 39\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/theco/Documents/Phishing-ML/classif.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plot_hist(d2_neg_counts, \u001b[39m10\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdfSpam (stopwords removed) - non-Spam emails\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\theco\\Documents\\Phishing-ML\\classif.ipynb Cell 39\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/theco/Documents/Phishing-ML/classif.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot_hist\u001b[39m(counter, n, name):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/theco/Documents/Phishing-ML/classif.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     words, counts \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mcounter\u001b[39m.\u001b[39mmost_common(n)))\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/theco/Documents/Phishing-ML/classif.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     plt\u001b[39m.\u001b[39;49mhist(counts, words)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\matplotlib\\pyplot.py:2561\u001b[0m, in \u001b[0;36mhist\u001b[1;34m(x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, data, **kwargs)\u001b[0m\n\u001b[0;32m   2555\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mhist)\n\u001b[0;32m   2556\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhist\u001b[39m(\n\u001b[0;32m   2557\u001b[0m         x, bins\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39mrange\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, density\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, weights\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2558\u001b[0m         cumulative\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, bottom\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, histtype\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbar\u001b[39m\u001b[39m'\u001b[39m, align\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmid\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   2559\u001b[0m         orientation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m'\u001b[39m, rwidth\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, log\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, color\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   2560\u001b[0m         label\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, stacked\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 2561\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39mhist(\n\u001b[0;32m   2562\u001b[0m         x, bins\u001b[39m=\u001b[39mbins, \u001b[39mrange\u001b[39m\u001b[39m=\u001b[39m\u001b[39mrange\u001b[39m, density\u001b[39m=\u001b[39mdensity, weights\u001b[39m=\u001b[39mweights,\n\u001b[0;32m   2563\u001b[0m         cumulative\u001b[39m=\u001b[39mcumulative, bottom\u001b[39m=\u001b[39mbottom, histtype\u001b[39m=\u001b[39mhisttype,\n\u001b[0;32m   2564\u001b[0m         align\u001b[39m=\u001b[39malign, orientation\u001b[39m=\u001b[39morientation, rwidth\u001b[39m=\u001b[39mrwidth, log\u001b[39m=\u001b[39mlog,\n\u001b[0;32m   2565\u001b[0m         color\u001b[39m=\u001b[39mcolor, label\u001b[39m=\u001b[39mlabel, stacked\u001b[39m=\u001b[39mstacked,\n\u001b[0;32m   2566\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m: data} \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m {}), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\matplotlib\\__init__.py:1423\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m   1421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1422\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1423\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39m\u001b[39mmap\u001b[39m(sanitize_sequence, args), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1425\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1426\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[0;32m   1427\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\matplotlib\\axes\\_axes.py:6637\u001b[0m, in \u001b[0;36mAxes.hist\u001b[1;34m(self, x, bins, range, density, weights, cumulative, bottom, histtype, align, orientation, rwidth, log, color, label, stacked, **kwargs)\u001b[0m\n\u001b[0;32m   6634\u001b[0m     bin_range \u001b[39m=\u001b[39m convert_units(bin_range)\n\u001b[0;32m   6636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m cbook\u001b[39m.\u001b[39mis_scalar_or_string(bins):\n\u001b[1;32m-> 6637\u001b[0m     bins \u001b[39m=\u001b[39m convert_units(bins)\n\u001b[0;32m   6639\u001b[0m \u001b[39m# We need to do to 'weights' what was done to 'x'\u001b[39;00m\n\u001b[0;32m   6640\u001b[0m \u001b[39mif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\matplotlib\\artist.py:251\u001b[0m, in \u001b[0;36mArtist.convert_xunits\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mif\u001b[39;00m ax \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m ax\u001b[39m.\u001b[39mxaxis \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    250\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m--> 251\u001b[0m \u001b[39mreturn\u001b[39;00m ax\u001b[39m.\u001b[39;49mxaxis\u001b[39m.\u001b[39;49mconvert_units(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\matplotlib\\axis.py:1667\u001b[0m, in \u001b[0;36mAxis.convert_units\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1665\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconverter\u001b[39m.\u001b[39mconvert(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits, \u001b[39mself\u001b[39m)\n\u001b[0;32m   1666\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m-> 1667\u001b[0m     \u001b[39mraise\u001b[39;00m munits\u001b[39m.\u001b[39mConversionError(\u001b[39m'\u001b[39m\u001b[39mFailed to convert value(s) to axis \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1668\u001b[0m                                  \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39munits: \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m!r}\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1669\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[1;31mConversionError\u001b[0m: Failed to convert value(s) to axis units: ('_', 'language', 'university', 'subject', '1', 'http', 'enron', 'com', 'e', '2')"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGiCAYAAADA0E3hAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcw0lEQVR4nO3db2zdVf3A8U/b0VsItEzn2m0WKyiiAhturBYkiKk2gUz3wDjBbHPhj+AkuEZlY7CK6DoRyKIrLkwQH6ibEDDGLUOsLgapWdjWBGSDwMBNYwsT184iLWu/vweG+qvrYLf0z077eiX3wY7n3O+5Hkbf3H8tyLIsCwCABBSO9QYAAI6VcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSkXe4/OEPf4h58+bF9OnTo6CgIH75y1++5Zpt27bFRz7ykcjlcvG+970v7r///iFsFQCY6PIOl66urpg5c2Y0NTUd0/wXXnghLrvssrjkkkuitbU1vvrVr8ZVV10VjzzySN6bBQAmtoK380sWCwoK4uGHH4758+cfdc6NN94Ymzdvjqeeeqp/7POf/3wcPHgwtm7dOtRLAwAT0KSRvkBLS0vU1tYOGKurq4uvfvWrR13T3d0d3d3d/X/u6+uLV155Jd75zndGQUHBSG0VABhGWZbFoUOHYvr06VFYODxvqx3xcGlra4vy8vIBY+Xl5dHZ2Rn//ve/48QTTzxiTWNjY9x6660jvTUAYBTs378/3v3udw/LfY14uAzFihUror6+vv/PHR0dcdppp8X+/fujtLR0DHcGAByrzs7OqKysjFNOOWXY7nPEw6WioiLa29sHjLW3t0dpaemgz7ZERORyucjlckeMl5aWChcASMxwvs1jxL/HpaamJpqbmweMPfroo1FTUzPSlwYAxpm8w+Vf//pXtLa2Rmtra0T85+POra2tsW/fvoj4z8s8ixYt6p9/7bXXxt69e+Mb3/hG7NmzJ+6+++74xS9+EcuWLRueRwAATBh5h8sTTzwR5513Xpx33nkREVFfXx/nnXderFq1KiIi/v73v/dHTETEe9/73ti8eXM8+uijMXPmzLjzzjvjRz/6UdTV1Q3TQwAAJoq39T0uo6WzszPKysqio6PDe1wAIBEj8fPb7yoCAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZQwqXpqamqKqqipKSkqiuro7t27e/6fy1a9fGBz7wgTjxxBOjsrIyli1bFq+99tqQNgwATFx5h8umTZuivr4+GhoaYufOnTFz5syoq6uLl156adD5P/vZz2L58uXR0NAQu3fvjnvvvTc2bdoUN91009vePAAwseQdLnfddVdcffXVsWTJkvjQhz4U69evj5NOOinuu+++Qec//vjjceGFF8YVV1wRVVVV8alPfSouv/zyt3yWBgDgf+UVLj09PbFjx46ora397x0UFkZtbW20tLQMuuaCCy6IHTt29IfK3r17Y8uWLXHppZce9Trd3d3R2dk54AYAMCmfyQcOHIje3t4oLy8fMF5eXh579uwZdM0VV1wRBw4ciI997GORZVkcPnw4rr322jd9qaixsTFuvfXWfLYGAEwAI/6pom3btsXq1avj7rvvjp07d8ZDDz0Umzdvjttuu+2oa1asWBEdHR39t/3794/0NgGABOT1jMuUKVOiqKgo2tvbB4y3t7dHRUXFoGtuueWWWLhwYVx11VUREXHOOedEV1dXXHPNNbFy5cooLDyynXK5XORyuXy2BgBMAHk941JcXByzZ8+O5ubm/rG+vr5obm6OmpqaQde8+uqrR8RJUVFRRERkWZbvfgGACSyvZ1wiIurr62Px4sUxZ86cmDt3bqxduza6urpiyZIlERGxaNGimDFjRjQ2NkZExLx58+Kuu+6K8847L6qrq+O5556LW265JebNm9cfMAAAxyLvcFmwYEG8/PLLsWrVqmhra4tZs2bF1q1b+9+wu2/fvgHPsNx8881RUFAQN998c/ztb3+Ld73rXTFv3rz4zne+M3yPAgCYEAqyBF6v6ezsjLKysujo6IjS0tKx3g4AcAxG4ue331UEACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhhQuTU1NUVVVFSUlJVFdXR3bt29/0/kHDx6MpUuXxrRp0yKXy8WZZ54ZW7ZsGdKGAYCJa1K+CzZt2hT19fWxfv36qK6ujrVr10ZdXV0888wzMXXq1CPm9/T0xCc/+cmYOnVqPPjggzFjxoz4y1/+Eqeeeupw7B8AmEAKsizL8llQXV0d559/fqxbty4iIvr6+qKysjKuv/76WL58+RHz169fH9/73vdiz549ccIJJwxpk52dnVFWVhYdHR1RWlo6pPsAAEbXSPz8zuulop6entixY0fU1tb+9w4KC6O2tjZaWloGXfOrX/0qampqYunSpVFeXh5nn312rF69Onp7e496ne7u7ujs7BxwAwDIK1wOHDgQvb29UV5ePmC8vLw82traBl2zd+/eePDBB6O3tze2bNkSt9xyS9x5553x7W9/+6jXaWxsjLKysv5bZWVlPtsEAMapEf9UUV9fX0ydOjXuueeemD17dixYsCBWrlwZ69evP+qaFStWREdHR/9t//79I71NACABeb05d8qUKVFUVBTt7e0Dxtvb26OiomLQNdOmTYsTTjghioqK+sc++MEPRltbW/T09ERxcfERa3K5XORyuXy2BgBMAHk941JcXByzZ8+O5ubm/rG+vr5obm6OmpqaQddceOGF8dxzz0VfX1//2LPPPhvTpk0bNFoAAI4m75eK6uvrY8OGDfGTn/wkdu/eHdddd110dXXFkiVLIiJi0aJFsWLFiv751113Xbzyyitxww03xLPPPhubN2+O1atXx9KlS4fvUQAAE0Le3+OyYMGCePnll2PVqlXR1tYWs2bNiq1bt/a/YXffvn1RWPjfHqqsrIxHHnkkli1bFueee27MmDEjbrjhhrjxxhuH71EAABNC3t/jMhZ8jwsApGfMv8cFAGAsCRcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIxpDCpampKaqqqqKkpCSqq6tj+/btx7Ru48aNUVBQEPPnzx/KZQGACS7vcNm0aVPU19dHQ0ND7Ny5M2bOnBl1dXXx0ksvvem6F198Mb72ta/FRRddNOTNAgATW97hctddd8XVV18dS5YsiQ996EOxfv36OOmkk+K+++476pre3t74whe+ELfeemucfvrpb3mN7u7u6OzsHHADAMgrXHp6emLHjh1RW1v73zsoLIza2tpoaWk56rpvfetbMXXq1LjyyiuP6TqNjY1RVlbWf6usrMxnmwDAOJVXuBw4cCB6e3ujvLx8wHh5eXm0tbUNuuaxxx6Le++9NzZs2HDM11mxYkV0dHT03/bv35/PNgGAcWrSSN75oUOHYuHChbFhw4aYMmXKMa/L5XKRy+VGcGcAQIryCpcpU6ZEUVFRtLe3Dxhvb2+PioqKI+Y///zz8eKLL8a8efP6x/r6+v5z4UmT4plnnokzzjhjKPsGACagvF4qKi4ujtmzZ0dzc3P/WF9fXzQ3N0dNTc0R888666x48skno7W1tf/26U9/Oi655JJobW313hUAIC95v1RUX18fixcvjjlz5sTcuXNj7dq10dXVFUuWLImIiEWLFsWMGTOisbExSkpK4uyzzx6w/tRTT42IOGIcAOCt5B0uCxYsiJdffjlWrVoVbW1tMWvWrNi6dWv/G3b37dsXhYW+kBcAGH4FWZZlY72Jt9LZ2RllZWXR0dERpaWlY70dAOAYjMTPb0+NAADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJAM4QIAJEO4AADJEC4AQDKECwCQjCGFS1NTU1RVVUVJSUlUV1fH9u3bjzp3w4YNcdFFF8XkyZNj8uTJUVtb+6bzAQCOJu9w2bRpU9TX10dDQ0Ps3LkzZs6cGXV1dfHSSy8NOn/btm1x+eWXx+9///toaWmJysrK+NSnPhV/+9vf3vbmAYCJpSDLsiyfBdXV1XH++efHunXrIiKir68vKisr4/rrr4/ly5e/5fre3t6YPHlyrFu3LhYtWjTonO7u7uju7u7/c2dnZ1RWVkZHR0eUlpbms10AYIx0dnZGWVnZsP78zusZl56entixY0fU1tb+9w4KC6O2tjZaWlqO6T5effXVeP311+Md73jHUec0NjZGWVlZ/62ysjKfbQIA41Re4XLgwIHo7e2N8vLyAePl5eXR1tZ2TPdx4403xvTp0wfEz/9asWJFdHR09N/279+fzzYBgHFq0mhebM2aNbFx48bYtm1blJSUHHVeLpeLXC43ijsDAFKQV7hMmTIlioqKor29fcB4e3t7VFRUvOnaO+64I9asWRO//e1v49xzz81/pwDAhJfXS0XFxcUxe/bsaG5u7h/r6+uL5ubmqKmpOeq622+/PW677bbYunVrzJkzZ+i7BQAmtLxfKqqvr4/FixfHnDlzYu7cubF27dro6uqKJUuWRETEokWLYsaMGdHY2BgREd/97ndj1apV8bOf/Syqqqr63wtz8sknx8knnzyMDwUAGO/yDpcFCxbEyy+/HKtWrYq2traYNWtWbN26tf8Nu/v27YvCwv8+kfPDH/4wenp64rOf/eyA+2loaIhvfvObb2/3AMCEkvf3uIyFkfgcOAAwssb8e1wAAMaScAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkCBcAIBnCBQBIhnABAJIhXACAZAgXACAZwgUASIZwAQCSIVwAgGQIFwAgGcIFAEiGcAEAkiFcAIBkDClcmpqaoqqqKkpKSqK6ujq2b9/+pvMfeOCBOOuss6KkpCTOOeec2LJly5A2CwBMbHmHy6ZNm6K+vj4aGhpi586dMXPmzKirq4uXXnpp0PmPP/54XH755XHllVfGrl27Yv78+TF//vx46qmn3vbmAYCJpSDLsiyfBdXV1XH++efHunXrIiKir68vKisr4/rrr4/ly5cfMX/BggXR1dUVv/71r/vHPvrRj8asWbNi/fr1g16ju7s7uru7+//c0dERp512Wuzfvz9KS0vz2S4AMEY6OzujsrIyDh48GGVlZcNyn5PymdzT0xM7duyIFStW9I8VFhZGbW1ttLS0DLqmpaUl6uvrB4zV1dXFL3/5y6Nep7GxMW699dYjxisrK/PZLgBwHPjHP/4xNuFy4MCB6O3tjfLy8gHj5eXlsWfPnkHXtLW1DTq/ra3tqNdZsWLFgNg5ePBgvOc974l9+/YN2wNnaN6oZ89+jT1ncfxwFscX53H8eOMVk3e84x3Ddp95hctoyeVykcvljhgvKyvzD+FxorS01FkcJ5zF8cNZHF+cx/GjsHD4PsSc1z1NmTIlioqKor29fcB4e3t7VFRUDLqmoqIir/kAAEeTV7gUFxfH7Nmzo7m5uX+sr68vmpubo6amZtA1NTU1A+ZHRDz66KNHnQ8AcDR5v1RUX18fixcvjjlz5sTcuXNj7dq10dXVFUuWLImIiEWLFsWMGTOisbExIiJuuOGGuPjii+POO++Myy67LDZu3BhPPPFE3HPPPcd8zVwuFw0NDYO+fMTochbHD2dx/HAWxxfncfwYibPI++PQERHr1q2L733ve9HW1hazZs2K73//+1FdXR0RER//+Mejqqoq7r///v75DzzwQNx8883x4osvxvvf//64/fbb49JLLx22BwEATAxDChcAgLHgdxUBAMkQLgBAMoQLAJAM4QIAJOO4CZempqaoqqqKkpKSqK6uju3bt7/p/AceeCDOOuusKCkpiXPOOSe2bNkySjsd//I5iw0bNsRFF10UkydPjsmTJ0dtbe1bnh3HLt+/F2/YuHFjFBQUxPz580d2gxNIvmdx8ODBWLp0aUybNi1yuVyceeaZ/j01TPI9i7Vr18YHPvCBOPHEE6OysjKWLVsWr7322ijtdvz6wx/+EPPmzYvp06dHQUHBm/4Owjds27YtPvKRj0Qul4v3ve99Az6BfMyy48DGjRuz4uLi7L777sv+/Oc/Z1dffXV26qmnZu3t7YPO/+Mf/5gVFRVlt99+e/b0009nN998c3bCCSdkTz755CjvfPzJ9yyuuOKKrKmpKdu1a1e2e/fu7Itf/GJWVlaW/fWvfx3lnY8/+Z7FG1544YVsxowZ2UUXXZR95jOfGZ3NjnP5nkV3d3c2Z86c7NJLL80ee+yx7IUXXsi2bduWtba2jvLOx598z+KnP/1plsvlsp/+9KfZCy+8kD3yyCPZtGnTsmXLlo3yzsefLVu2ZCtXrsweeuihLCKyhx9++E3n7927NzvppJOy+vr67Omnn85+8IMfZEVFRdnWrVvzuu5xES5z587Nli5d2v/n3t7ebPr06VljY+Og8z/3uc9ll1122YCx6urq7Etf+tKI7nMiyPcs/tfhw4ezU045JfvJT34yUlucMIZyFocPH84uuOCC7Ec/+lG2ePFi4TJM8j2LH/7wh9npp5+e9fT0jNYWJ4x8z2Lp0qXZJz7xiQFj9fX12YUXXjii+5xojiVcvvGNb2Qf/vCHB4wtWLAgq6ury+taY/5SUU9PT+zYsSNqa2v7xwoLC6O2tjZaWloGXdPS0jJgfkREXV3dUedzbIZyFv/r1Vdfjddff31YfxPoRDTUs/jWt74VU6dOjSuvvHI0tjkhDOUsfvWrX0VNTU0sXbo0ysvL4+yzz47Vq1dHb2/vaG17XBrKWVxwwQWxY8eO/peT9u7dG1u2bPElqGNguH52j/lvhz5w4ED09vZGeXn5gPHy8vLYs2fPoGva2toGnd/W1jZi+5wIhnIW/+vGG2+M6dOnH/EPJ/kZylk89thjce+990Zra+so7HDiGMpZ7N27N373u9/FF77whdiyZUs899xz8eUvfzlef/31aGhoGI1tj0tDOYsrrrgiDhw4EB/72Mciy7I4fPhwXHvttXHTTTeNxpb5f472s7uzszP+/e9/x4knnnhM9zPmz7gwfqxZsyY2btwYDz/8cJSUlIz1diaUQ4cOxcKFC2PDhg0xZcqUsd7OhNfX1xdTp06Ne+65J2bPnh0LFiyIlStXxvr168d6axPOtm3bYvXq1XH33XfHzp0746GHHorNmzfHbbfdNtZbY4jG/BmXKVOmRFFRUbS3tw8Yb29vj4qKikHXVFRU5DWfYzOUs3jDHXfcEWvWrInf/va3ce65547kNieEfM/i+eefjxdffDHmzZvXP9bX1xcREZMmTYpnnnkmzjjjjJHd9Dg1lL8X06ZNixNOOCGKior6xz74wQ9GW1tb9PT0RHFx8YjuebwaylnccsstsXDhwrjqqqsiIuKcc86Jrq6uuOaaa2LlypVRWOi/30fL0X52l5aWHvOzLRHHwTMuxcXFMXv27Ghubu4f6+vri+bm5qipqRl0TU1NzYD5ERGPPvroUedzbIZyFhERt99+e9x2222xdevWmDNnzmhsddzL9yzOOuusePLJJ6O1tbX/9ulPfzouueSSaG1tjcrKytHc/rgylL8XF154YTz33HP98RgR8eyzz8a0adNEy9swlLN49dVXj4iTN4Iy86v6RtWw/ezO733DI2Pjxo1ZLpfL7r///uzpp5/OrrnmmuzUU0/N2trasizLsoULF2bLly/vn//HP/4xmzRpUnbHHXdku3fvzhoaGnwcepjkexZr1qzJiouLswcffDD7+9//3n87dOjQWD2EcSPfs/hfPlU0fPI9i3379mWnnHJK9pWvfCV75plnsl//+tfZ1KlTs29/+9tj9RDGjXzPoqGhITvllFOyn//859nevXuz3/zmN9kZZ5yRfe5znxurhzBuHDp0KNu1a1e2a9euLCKyu+66K9u1a1f2l7/8JcuyLFu+fHm2cOHC/vlvfBz661//erZ79+6sqakp3Y9DZ1mW/eAHP8hOO+20rLi4OJs7d272pz/9qf9/u/jii7PFixcPmP+LX/wiO/PMM7Pi4uLswx/+cLZ58+ZR3vH4lc9ZvOc978ki4ohbQ0PD6G98HMr378X/J1yGV75n8fjjj2fV1dVZLpfLTj/99Ow73/lOdvjw4VHe9fiUz1m8/vrr2Te/+c3sjDPOyEpKSrLKysrsy1/+cvbPf/5z9Dc+zvz+978f9N//b/z/v3jx4uziiy8+Ys2sWbOy4uLi7PTTT89+/OMf533dgizzXBkAkIYxf48LAMCxEi4AQDKECwCQDOECACRDuAAAyRAuAEAyhAsAkAzhAgAkQ7gAAMkQLgBAMoQLAJCM/wM9kKRvAVrZIAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_hist(d2_neg_counts, 10, \"dfSpam (stopwords removed) - non-Spam emails\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF\n",
    "\n",
    "Work in Progress: running TFIDF to classify Email-Spam-Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\theco\\Documents\\Phishing-ML\\classif.ipynb Cell 31\u001b[0m in \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/theco/Documents/Phishing-ML/classif.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/theco/Documents/Phishing-ML/classif.ipynb#X42sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tfidf \u001b[39m=\u001b[39m TfidfVectorizer(sublinear_tf\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, min_df\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, norm\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39ml2\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlatin-1\u001b[39m\u001b[39m'\u001b[39m, ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m), stop_words\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m, max_features\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/theco/Documents/Phishing-ML/classif.ipynb#X42sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m features \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39mfit_transform(df\u001b[39m.\u001b[39mppBody)\u001b[39m.\u001b[39mtoarray()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english', max_features=10000)\n",
    "features = tfidf.fit_transform(df.ppBody).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18651, 10000)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = d.pp_df.iloc[:, 0:2]\n",
    "y = d.pp_df.iloc[:, 2]\n",
    "\n",
    "ss = ShuffleSplit(n_splits=1, test_size=0.25, random_state=10)\n",
    "indexes = list(ss.split(X, y))\n",
    "train_set  = indexes[0][0]\n",
    "test_set  = indexes[0][1]\n",
    "Xtrain = X.iloc[train_set, :]\n",
    "ytrain = y.iloc[train_set]\n",
    "Xtest = X.iloc[test_set, :]\n",
    "ytest = y.iloc[test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Classify with word2vec (see https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "Classify with BERT (also using TDS article methods)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Use RF/ensemble method for some of the classifiers above to see which features work best and which preprocessing methods are good - exploring Phishing-Dataset-for-Machine-Learning features first may be helpful"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phishing-Dataset-for-Machine-Learning\n",
    "\n",
    "Inspect this dataset - it is for webpages (https://www.sciencedirect.com/science/article/pii/S0020025519300763?via%3Dihub) but some of the features may be applicable to email classification\n",
    "\n",
    "Other repos that use this dataset to take inspiration from:\n",
    "- https://github.com/andpalmier/MLWithPhishing\n",
    "- https://github.com/rewanthtammana/Detect-phishing-websites-using-ML\n",
    "- https://www.kaggle.com/code/fadilparves/pishing-detection-using-machine-learning\n",
    "\n",
    "Random forests used to discover best features - we could apply similar to the Email-Spam-Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Feature Extraction\n",
    "\n",
    "Can we apply the features used in this dataset to Email-Spam-Dataset?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "\n",
    "- [x] Remove NaN id from merged `dfSpam`\n",
    "- [ ] Tokenise body data\n",
    "- [ ] Set seed method\n",
    "- [ ] Stratified Sample?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
