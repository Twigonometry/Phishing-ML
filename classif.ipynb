{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Linear Regression\n",
    "\n",
    "Experimental notebook for classification of phishing emails using basic Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements - uncomment this line the first time you run this notebook\n",
    "#!pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_col_names = ['id', 'Body', 'Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSA = pd.read_csv('./kaggle-datasets/Email-Spam-Dataset/completeSpamAssassin.csv', names=spam_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfEnron = pd.read_csv('./kaggle-datasets/Email-Spam-Dataset/enronSpamSubset.csv', names=spam_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLing = pd.read_csv('./kaggle-datasets/Email-Spam-Dataset/lingSpam.csv', names=spam_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge datasets\n",
    "dfs = [dfSA, dfEnron, dfLing]\n",
    "dfSpam = pd.concat(dfs).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>\\nSave up to 70% on Life Insurance.\\nWhy Spend...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>1) Fight The Risk of Cancer!\\nhttp://www.adcli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>##############################################...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I thought you might like these:\\n1) Slim Down ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               Body Label\n",
       "1.0  0.0  \\nSave up to 70% on Life Insurance.\\nWhy Spend...     1\n",
       "2.0  1.0  1) Fight The Risk of Cancer!\\nhttp://www.adcli...     1\n",
       "3.0  2.0  1) Fight The Risk of Cancer!\\nhttp://www.adcli...     1\n",
       "4.0  3.0  ##############################################...     1\n",
       "5.0  4.0  I thought you might like these:\\n1) Slim Down ...     1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSpam.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "In avoidance of 'self-plagiarisation', much of the code for the `Dataset` class is adapted from my Text Processing Sentiment Analysis assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords as NLTK_STOP\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download wordnet for lemmatization\n",
    "#uncomment appropriate line if you get error: \"Resource wordnet not found.\", \"Resource punkt not found.\", etc...\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy imports\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOP\n",
    "\n",
    "#download spacy dataset\n",
    "#uncomment the line below if you get error \"Can't find model 'en_core_web_sm'\"\n",
    "# ! python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, df, preprocessing=[\"lower\"], feature_selection=[\"alltokens\"]):\n",
    "        \n",
    "        #read preprocessing and feature_selection configuration\n",
    "        self.preprocessing = preprocessing\n",
    "        self.feature_selection = feature_selection\n",
    "        self.data = df\n",
    "\n",
    "        # === define various processors and regexes for various preprocessing/feature selection methods ===\n",
    "\n",
    "        # NLTK Stemming Engine\n",
    "        self.porter = PorterStemmer()\n",
    "\n",
    "        #NLTK Lemmatizing Engine\n",
    "        self.wn_lt = WordNetLemmatizer()\n",
    "\n",
    "        #tokenizers\n",
    "\n",
    "        #words regex - splits on word boundaries, doesn't include punctuation etc\n",
    "        self.word_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "        #create tokenizer based on NLTK-provided regex from Labs\n",
    "        nltk_pat = r'''(?x) # set flag to allow verbose regexps\n",
    "            (?:[A-Z]\\.)+ # abbreviations, e.g. U.S.A.\n",
    "            | \\w+(?:-\\w+)* # words with optional internal hyphens\n",
    "            | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "            | \\.\\.\\. # ellipsis\n",
    "            | [][.,;\"'?():_`-]\n",
    "            | [>]?[:;][\\']?[\\(\\)\\[\\]]+ # these are separate tokens; includes ], [\n",
    "            '''\n",
    "        self.nltk_tokenizer = RegexpTokenizer(nltk_pat)\n",
    "\n",
    "        #create tokenizer based on custom regex based on above, with less features\n",
    "        custom_pat = r'''(?x)\n",
    "                \\w+(?:-\\w+)*\n",
    "                |\\$?\\d+(?:\\.\\d+)?%?\n",
    "                |\\.\\.\\.'''\n",
    "        self.custom_tokenizer = RegexpTokenizer(custom_pat)\n",
    "\n",
    "        # stop words - SPACY_STOP defined above in imports\n",
    "\n",
    "        self.NLTK_ENGLISH_STOP = set(NLTK_STOP.words('english'))\n",
    "\n",
    "        # === apply preprocessing and feature selection ===\n",
    "\n",
    "        self.process_phrases()\n",
    "    \n",
    "    def preprocess_phrase(self, phrase):\n",
    "        \"\"\"define preprocessing function for phrases\n",
    "        can call any number of these options - however, some may not combine well\"\"\"\n",
    "\n",
    "        if self.preprocessing == []:\n",
    "            #no preprocessing\n",
    "            return phrase\n",
    "        if \"lower\" in self.preprocessing:\n",
    "            #lowercase\n",
    "            phrase = phrase.lower()\n",
    "        if \"newlines\" in self.preprocessing:\n",
    "            phrase = \" \".join(filter(None, phrase.split(\"\\n\")))\n",
    "        if \"punc\" in self.preprocessing:\n",
    "            #remove punctuation\n",
    "            phrase = phrase.translate(str.maketrans('','',string.punctuation))\n",
    "        if \"stemming\" in self.preprocessing:\n",
    "            #use NLTK stemming\n",
    "            phrase = self.porter.stem(phrase)\n",
    "        if \"nltk_lemmatize\" in self.preprocessing:\n",
    "            #use NLTK's lemmatization method\n",
    "            new_phrase = \"\"\n",
    "            words = nltk.word_tokenize(phrase)\n",
    "            for word in words:\n",
    "                new_phrase += self.wn_lt.lemmatize(word)\n",
    "            phrase = new_phrase\n",
    "        if \"spacy_lemmatize\" in self.preprocessing:\n",
    "            #use spacy's lemmatization method\n",
    "            nlp_phrase = nlp(phrase)\n",
    "            new_phrase = \"\"\n",
    "            for token in nlp_phrase:\n",
    "                new_phrase += (token.lemma_ + \" \")\n",
    "            phrase = new_phrase\n",
    "\n",
    "        return phrase\n",
    "\n",
    "    def extract_features_from_phrase(self, phrase):\n",
    "        \"\"\"define feature extraction function for phrases\n",
    "        extracts all words from all phrases as features for document set\n",
    "        for each method, if one has already been applied the phrase must be treated as a list\"\"\"\n",
    "\n",
    "        #list of negation words from \n",
    "        negation_words = [\"neither\", \"never\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"nowhere\"]\n",
    "\n",
    "        intensifer_words = [\"absolutely\", \"completely\", \"extremely\", \"highly\", \"rather\", \"really\", \"so\", \"too\", \"totally\", \"utterly\", \"very\"]\n",
    "\n",
    "        #use if len(self.feature_selection) > 1 to check if any preprocessing already occurred, as input will be in a list\n",
    "\n",
    "        if self.feature_selection == []:\n",
    "            #we have to do some feature selection, absolute minimum is alltokens\n",
    "            return phrase.split(\" \")\n",
    "        if \"alltokens\" in self.feature_selection:\n",
    "            #split based on whitespace\n",
    "            phrase = phrase.split(\" \")\n",
    "        if \"nltk_tokenize\" in self.feature_selection:\n",
    "            #tokenize using NLTK tokenizer, with words regex\n",
    "            if len(self.feature_selection) > 1:\n",
    "                phrase =  list(itertools.chain.from_iterable([self.word_tokenizer.tokenize(partial_phrase) for partial_phrase in phrase]))\n",
    "            else:\n",
    "                phrase = self.word_tokenizer.tokenize(phrase)\n",
    "        if \"nltk_tokenize_2\" in self.feature_selection:\n",
    "            #tokenize using NLTK's regex\n",
    "            if len(self.feature_selection) > 1:\n",
    "                phrase =  list(itertools.chain.from_iterable([self.nltk_tokenizer.tokenize(partial_phrase) for partial_phrase in phrase]))\n",
    "            else:\n",
    "                phrase = self.nltk_tokenizer.tokenize(phrase)\n",
    "        if \"custom_tokenize\" in self.feature_selection:\n",
    "            #tokenize with custom regex\n",
    "            if len(self.feature_selection) > 1:\n",
    "                phrase =  list(itertools.chain.from_iterable([self.custom_tokenizer.tokenize(partial_phrase) for partial_phrase in phrase]))\n",
    "            else:\n",
    "                phrase = self.custom_tokenizer.tokenize(phrase)\n",
    "        if \"nltk_stoplist\" in self.feature_selection:\n",
    "            #use an NLTK stoplist\n",
    "            if len(self.feature_selection) > 1:\n",
    "                phrases = []\n",
    "                for partial_phrase in phrase:\n",
    "                    phrases +=  [word for word in partial_phrase.split(\" \") if word not in self.NLTK_ENGLISH_STOP]\n",
    "                phrase = list(itertools.chain.from_iterable(phrases))\n",
    "            else:\n",
    "                phrase = [word for word in phrase.split(\" \") if word not in self.NLTK_ENGLISH_STOP]\n",
    "        if \"spacy_stoplist\" in self.feature_selection:\n",
    "            #use a spacy stoplist\n",
    "            if len(self.feature_selection) > 1:\n",
    "                phrases = []\n",
    "                for partial_phrase in phrase:\n",
    "                    phrases +=  [word for word in partial_phrase.split(\" \") if word not in list(SPACY_STOP)]\n",
    "                phrase = list(itertools.chain.from_iterable(phrases))\n",
    "            else:\n",
    "                phrase = [word for word in phrase.split(\" \") if word not in list(SPACY_STOP)]\n",
    "        if \"nltk_pos_tag\" in self.feature_selection:\n",
    "            #use POS tagging - must be a list, so split by spaces if not already\n",
    "            if isinstance(phrase, list):\n",
    "                phrase = nltk.pos_tag(phrase)\n",
    "            else:\n",
    "                phrase = phrase.split(\" \")\n",
    "                phrase = nltk.pos_tag(phrase)\n",
    "        if \"negation_bigrams\" in self.feature_selection:\n",
    "            #create bigrams with negation words + their successors\n",
    "            if len(self.feature_selection) > 1:\n",
    "                phrases = []\n",
    "                for j in range(0, len(phrase)):\n",
    "                    partial_phrase = phrase[j]\n",
    "                    split = partial_phrase.split(\" \")\n",
    "\n",
    "                    for i in range(0, len(split)):\n",
    "                        if split[i] in negation_words and i != len(split) - 1:\n",
    "                            #add the negation word and its successor to the list of phrases\n",
    "                            phrases.append(split[i] + \" \" + split[i+1])\n",
    "                        else:\n",
    "                            #skip to next partial if negation is at end of partial phrase\n",
    "                            if split[i] in negation_words and j != len(phrase) - 1:\n",
    "                                # print(\"Next partial\")\n",
    "                                phrases.append(split[i] + \" \" + phrase[j+1].split(\" \")[0])\n",
    "                            else:\n",
    "                                #skip entirely if this is the last partial phrase\n",
    "                                phrases.append(split[i])\n",
    "                phrase = phrases\n",
    "            else:\n",
    "                split = phrase.split(\" \")\n",
    "                phrase = []\n",
    "                for i in range(0, len(split)):\n",
    "                    if split[i] in negation_words and i != len(split) - 1:\n",
    "                        phrase.append(split[i] + \" \" + split[i+1])\n",
    "                    else:\n",
    "                        phrase.append(split[i])\n",
    "        if \"intensifier_bigrams\" in self.feature_selection:\n",
    "            #create bigrams with intensifier words + their successors\n",
    "            if len(self.feature_selection) > 1:\n",
    "                phrases = []\n",
    "                for j in range(0, len(phrase)):\n",
    "                    partial_phrase = phrase[j]\n",
    "                    split = partial_phrase.split(\" \")\n",
    "\n",
    "                    for i in range(0, len(split)):\n",
    "                        if split[i] in intensifer_words and i != len(split) - 1:\n",
    "                            #add the intensifier word and its successor to the list of phrases\n",
    "                            phrases.append(split[i] + \" \" + split[i+1])\n",
    "                        else:\n",
    "                            #skip to next partial if intensifier is at end of partial phrase\n",
    "                            if split[i] in intensifer_words and  j != len(phrase) - 1:\n",
    "                                phrases.append(split[i] + phrase[j+1].split(\" \")[0])\n",
    "                            else:\n",
    "                                #skip entirely if this is the last partial phrase\n",
    "                                phrases.append(split[i])\n",
    "                phrase = phrases\n",
    "            else:\n",
    "                split = phrase.split(\" \")\n",
    "                phrase = []\n",
    "                for i in range(0, len(split)):\n",
    "                    if split[i] in intensifer_words and i != len(split) - 1:\n",
    "                        phrase.append(split[i] + \" \" + split[i+1])\n",
    "                    else:\n",
    "                        phrase.append(split[i])\n",
    "\n",
    "        return phrase\n",
    "\n",
    "    def process_phrases(self):\n",
    "        \"\"\"extract bodies\"\"\"\n",
    "\n",
    "        self.bodies = self.data['Body']\n",
    "        \n",
    "        #apply preprocessing function to all phrases using list comprehension\n",
    "        self.preprocessed_phrases = [self.preprocess_phrase(phrase) for phrase in self.data['Body']]\n",
    "        self.features = [self.extract_features_from_phrase(phrase) for phrase in self.preprocessed_phrases]\n",
    "\n",
    "        data = {'id': self.data['id'], 'ppBody': self.features, 'label': self.data['Label']}\n",
    "        self.pp_df = pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ppBody</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>[save, up, to, 70%, on, life, insurance., why,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>[1), fight, the, risk, of, cancer!, http://www...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>[1), fight, the, risk, of, cancer!, http://www...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>[#############################################...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>[i, thought, you, might, like, these:, 1), sli...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                             ppBody label\n",
       "1.0  0.0  [save, up, to, 70%, on, life, insurance., why,...     1\n",
       "2.0  1.0  [1), fight, the, risk, of, cancer!, http://www...     1\n",
       "3.0  2.0  [1), fight, the, risk, of, cancer!, http://www...     1\n",
       "4.0  3.0  [#############################################...     1\n",
       "5.0  4.0  [i, thought, you, might, like, these:, 1), sli...     1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = Dataset(dfSpam, preprocessing=[\"lower\",\"newlines\"])\n",
    "d.pp_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "\n",
    "- [x] Remove NaN id from merged `dfSpam`\n",
    "- [ ] Tokenise body data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
